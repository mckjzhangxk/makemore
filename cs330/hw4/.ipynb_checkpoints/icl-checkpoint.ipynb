{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cProfile import label\n",
    "from multiprocessing.sharedctypes import Value\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import transformers\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import argparse\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "from rouge_score import rouge_scorer\n",
    "import tqdm\n",
    "\n",
    "import utils\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--task')\n",
    "parser.add_argument('--model')\n",
    "parser.add_argument('--dataset')\n",
    "parser.add_argument('--k')\n",
    "parser.add_argument('--prompt', default='qa')\n",
    "parser.add_argument('--debug', action='store_true')\n",
    "parser.add_argument('--repeats', default=1, type=int)\n",
    "parser.add_argument('--device', default='cuda')\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "DEVICE = torch.device(args.device)\n",
    "\n",
    "\n",
    "def get_icl_prompts(\n",
    "    support_inputs: List[str],\n",
    "    support_labels: List[str],\n",
    "    test_input: str,\n",
    "    prompt_mode: str = 'qa') -> str:\n",
    "    \"\"\"\n",
    "    Take a list of contexts and combine them into k-shot prompts.\n",
    "\n",
    "    **Note**: Be sure to shuffle the support examples and labels \n",
    "      *together* (i.e. so the pairings of support input/label is preserved)\n",
    "      before constructing the prompt. np.random.permutation may be helpful.\n",
    "\n",
    "    Args:\n",
    "      support_inputs: The k inputs used for in-context learning (k may be zero!)\n",
    "      support_labels: The k labels used for in-context learning (k may be zero!)\n",
    "      test_input: The input we are evaluating on\n",
    "      prompt_mode: The task description mode we're using; 'none' means we're only using\n",
    "        k-shot examples, 'tl;dr' means we're using the tl;dr prompt from the GPT-2 paper,\n",
    "        etc.\n",
    "\n",
    "    Returns:\n",
    "      A string containing the complete input to the model.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    prompt = ''\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def get_performance_metric(predictions: List[str], targets: List[str], metric: str) -> float:\n",
    "    if metric == 'rouge':\n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
    "        scores = []\n",
    "        for p, t in zip(predictions, targets):\n",
    "            score = scorer.score(p, t)['rouge1'].fmeasure\n",
    "            scores.append(score)\n",
    "        return sum(scores) / len(scores)\n",
    "    elif metric == 'exact match':\n",
    "        if isinstance(targets[0], str):\n",
    "            return sum([p.strip() == t.strip() for p, t in zip(predictions, targets)]) / len(predictions)\n",
    "        else:\n",
    "            def _normalize(prediction):\n",
    "                if prediction.endswith('Q'):\n",
    "                    prediction = prediction[:-1]\n",
    "                elif 'Q:' in prediction:\n",
    "                    prediction = prediction[:prediction.index('Q:')]\n",
    "                return prediction.strip('. ').lower()\n",
    "\n",
    "            normalized = [_normalize(p) for p in predictions]\n",
    "            def contains(key, candidates):\n",
    "                for c in candidates:\n",
    "                    if key in c:\n",
    "                        return True\n",
    "                return False\n",
    "\n",
    "            return sum([contains(n, t) for n, t in zip(normalized, targets)]) / len(normalized)\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "def do_sample(model, input_ids, stop_tokens, max_tokens):\n",
    "    \"\"\"\n",
    "    Sample from the model using the given input_ids as a prefix until we either\n",
    "    hit the stop token or we have sampled max_tokens tokens.\n",
    "\n",
    "    (Don't use model.generate; implement this yourself in a loop)\n",
    "\n",
    "    Note: when calling the model here, be sure to wrap the call with\n",
    "      torch.inferece_mode() to save memory!\n",
    "\n",
    "    Args:\n",
    "        model: A transformers.PreTrainedModel that we will sample from.\n",
    "        input_ids: An integer tensor of shape [1, prefix_len]\n",
    "        stop_tokens: A list of token ids that indicates that we should stop sampling (e.g., a period)\n",
    "        max_tokens: Stop sampling if we've sampled this many tokens\n",
    "    \n",
    "    Returns:\n",
    "        The sampled tokens (a python list of ints/zero-dim tensors), not including the input_ids prefix\n",
    "          OR the stop token (if we hit the stop token before max_tokens)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    sampled_tokens = []\n",
    "    return sampled_tokens\n",
    "\n",
    "\n",
    "def run_icl(models: List[str], datasets_: List[str], ks: List[int], prompt_modes: List[str], n_val: int = 125):\n",
    "    results = {}\n",
    "    for model_name in models:\n",
    "        print(f'Loading model {model_name}...')\n",
    "        model, tokenizer = utils.get_model_and_tokenizer(model_name, transformers.AutoModelForCausalLM)\n",
    "        stop_tokens = utils.stop_tokens(tokenizer)\n",
    "        model.to(DEVICE)\n",
    "\n",
    "        for dataset in datasets_:\n",
    "            print(f'Loading dataset {dataset}...')\n",
    "            if args.debug:\n",
    "                n_val = 1\n",
    "            max_tokens = utils.max_sampled_tokens_for_dataset(dataset)\n",
    "            train, val = utils.get_dataset(dataset, n_train=max(ks), n_val=n_val)\n",
    "            for prompt_mode in prompt_modes:\n",
    "                for k in ks:\n",
    "                    print(f'Running in-context learning with {model_name} on {dataset} with k={k} and prompt_mode={prompt_mode}')\n",
    "                    for repeat in range(args.repeats):\n",
    "                        if repeat > 0:\n",
    "                            print(f'Beginning repeat #{repeat}')\n",
    "                        support_idxs = random.choices(range(len(train['x'])), k=k)\n",
    "                        support_x = [train['x'][idx].replace('\\n', ' ') for idx in support_idxs]\n",
    "                        support_y = [train['simple_y'][idx].replace('\\n', ' ') for idx in support_idxs]\n",
    "                        targets = []\n",
    "                        predictions = []\n",
    "                        pbar = tqdm.tqdm(list(range(min(n_val, len(val['x'])))))\n",
    "                        for row in pbar:\n",
    "                            test_input = val['x'][row]\n",
    "                            targets.append(val['y'][row])\n",
    "\n",
    "                            # Ingredients you'll need:\n",
    "                            #   get_icl_prompts() [which you implemented]\n",
    "                            #   do_sample() [which you implemented]\n",
    "                            #   tokenizer() (for encoding text into tokens) and tokenizer.decode() (for decoding tokens back into text)\n",
    "                            #   See the documentation for the tokenizer encoder function here:\n",
    "                            #   https://huggingface.co/docs/transformers/v4.23.1/en/main_classes/tokenizer#transformers.PreTrainedTokenizer.__call__\n",
    "                            # Note that the tokenizer by default will give you results on the CPU, so you will need to move them to the\n",
    "                            # proper device.\n",
    "                            # YOUR CODE HERE\n",
    "\n",
    "                            decoded_prediction = ''\n",
    "\n",
    "                            # END YOUR CODE\n",
    "\n",
    "                            predictions.append(decoded_prediction)\n",
    "                            metric = get_performance_metric(predictions, targets, utils.metric_for_dataset(dataset))\n",
    "                            pbar.set_description(f'Eval: {metric:.04f}')\n",
    "                        results['_'.join([model_name, dataset, str(k), prompt_mode])] = metric\n",
    "\n",
    "                        print('Evaluation results:', results)\n",
    "                        if not os.path.exists('results/icl'):\n",
    "                            os.makedirs('results/icl')\n",
    "\n",
    "                        for k_, v in results.items():\n",
    "                            with open(f'results/icl/{k_}.json', 'w') as f:\n",
    "                                json.dump({'metric': v}, f)\n",
    "                        results = {}\n",
    "\n",
    "\n",
    "def plot(models, dataset, ks, prompt_modes):\n",
    "    data = defaultdict(lambda: defaultdict(list))\n",
    "    symbols = ['solid', 'dashed', 'dotted', 'dashdot']\n",
    "\n",
    "    x_vals = set()\n",
    "    for model in models:\n",
    "        symbol = symbols.pop(0)\n",
    "        for prompt_mode in prompt_modes:\n",
    "            for k in ks:\n",
    "                fn = '_'.join([model, dataset, str(k), prompt_mode])\n",
    "                id_ = '_'.join([model, dataset, prompt_mode])\n",
    "                with open(f'results/icl/{fn}.json', 'r') as f:\n",
    "                    score = json.load(f)['metric']\n",
    "                    data[id_]['x'].append(k)\n",
    "                    x_vals.add(k)\n",
    "                    data[id_]['y'].append(score)\n",
    "                    data[id_]['linestyle'] = symbol\n",
    "\n",
    "    for k, v in data.items():\n",
    "        plt.plot(v['x'], v['y'], label=k, linestyle=v['linestyle'])\n",
    "\n",
    "    if max(x_vals) > 4:\n",
    "        plt.xscale('symlog')\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    ax.xaxis.set_major_formatter(mticker.ScalarFormatter())\n",
    "    ax.xaxis.set_ticks(v['x'])\n",
    "    plt.legend()\n",
    "    plt.title(dataset)\n",
    "    plt.ylabel(utils.metric_for_dataset(dataset))\n",
    "    plt.xlabel('Number of support examples')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def run():\n",
    "    ks = [int(k) for k in args.k.split(',')]\n",
    "    if args.task == 'icl':\n",
    "        run_icl(args.model.split(','), args.dataset.split(','), ks, args.prompt.split(','))\n",
    "    elif args.task == 'plot':\n",
    "        assert ',' not in args.dataset, \"Only one dataset at a time for plotting\"\n",
    "        plot(args.model.split(','), args.dataset, ks, args.prompt.split(','))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import argparse\n",
    "import torch\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "import utils\n",
    "import copy\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import itertools\n",
    "import icl\n",
    "import tqdm\n",
    "import random\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--task')\n",
    "parser.add_argument('--model')\n",
    "parser.add_argument('--dataset')\n",
    "parser.add_argument('--k')\n",
    "parser.add_argument('--mode', default='all')\n",
    "parser.add_argument('--debug', action='store_true')\n",
    "parser.add_argument('--repeats', default=1, type=int)\n",
    "parser.add_argument('--device', default='cuda')\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "DEVICE = torch.device(args.device)\n",
    "\n",
    "\n",
    "class LoRAConv1DWrapper(nn.Module):\n",
    "    def __init__(self, conv1dmodule: nn.Module, lora_rank: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base_module = conv1dmodule\n",
    "\n",
    "        ###\n",
    "        ### Set up your LoRA-augmented layer here.\n",
    "        ### You should initialize your parameters so that the residual matrix AB^T is zero,\n",
    "        ###     but be careful how you do this (i.e., make sure you eventually get\n",
    "        ###     non-zero gradients to both matrices during fine-tuning)!\n",
    "        ### Initialization hint: what do the gradients look like after 1 and 2 steps of fine-tuning\n",
    "        ###     if you initialize both A and B to zero? What about if just one is zero?\n",
    "        ###\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    def forward(self, x):\n",
    "        ###\n",
    "        ### Perform the forward pass of your LoRA-augmented layer here.\n",
    "        ### Note: you don't need to ever explicitly construct the matrix AB^T.\n",
    "        ### Hint: matrix multiplication is associative.\n",
    "        ###\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "\n",
    "def parameters_to_fine_tune(model: nn.Module, mode: str) -> List:\n",
    "    \"\"\"\n",
    "    Select the parameters in `model` that should be fine-tuned in mode `mode`.\n",
    "\n",
    "    Args:\n",
    "      model: the model we're fine-tuning\n",
    "      mode: the fine-tuning mode we're using; may be 'all', 'last', 'first',\n",
    "        'middle', or 'loraN' (where N is an integer)\n",
    "    \n",
    "    Returns:\n",
    "      A list of nn.Parameters of `model` that should be fine-tuned in the given\n",
    "        fine-tuning mode.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    if mode == 'all':\n",
    "        pass\n",
    "    elif mode == 'last':\n",
    "        pass\n",
    "    elif mode == 'first':\n",
    "        pass\n",
    "    elif mode == 'middle':\n",
    "        pass\n",
    "    elif mode.startswith('lora'):\n",
    "        pass\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "def get_loss(logits: torch.tensor, targets: torch.tensor) -> torch.tensor:\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy loss for either sequence classification or generation.\n",
    "\n",
    "    For generation, you'll need to deal with the fact that different sequences witihn\n",
    "      the batch are different lengths, and the targets tensor includes some mask\n",
    "      values (-100). The average loss is the *average loss over all non-masked timesteps*.\n",
    "      You'll also need to handle the fact that the prediction for what token t will be is\n",
    "      made after seeing only t - 1 tokens; that is, there is an off-by-one shift needed\n",
    "      between the logits and targets.\n",
    "\n",
    "    Args:\n",
    "      logits: a 2D [batch_size, n_classes] (for classification) or 3D\n",
    "        [batch_size, sequence_length, vocab_size] (for generation) tensor\n",
    "        of *UNNORMALIZED* logits\n",
    "      targets: a 1D [batch_size] (for classification) or 2D [batch_size, sequence_length]\n",
    "        (for generation) tensor of target indices. For the generation case, may contain\n",
    "        -100 in some positions, meaning that the loss for this timestep should be ignored.\n",
    "    \n",
    "    Returns:\n",
    "      A zero-dim tensor representing the average cross-entropy loss over all batch \n",
    "        elements (and sequence timesteps, if applicable)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    if logits.dim() == 2:\n",
    "        pass\n",
    "    elif logits.dim() == 3:\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(f'Logits should either be 2-dim (for classification) or 3-dim (for generation); got {logits.dim()}')\n",
    "\n",
    "\n",
    "def get_acc(logits, targets):\n",
    "    \"\"\"\n",
    "    Computes the exact match accuracy for either sequence classification or generation. i.e.,\n",
    "      the fraction of predictions for which the most likely class/token equals the target.\n",
    "\n",
    "    For generation, you'll need to deal with the fact that different sequences witihn\n",
    "      the batch are different lengths, and the targets tensor includes some mask\n",
    "      values (-100). The average accuracy is the *average accuracy over all non-masked timesteps*.\n",
    "      You'll also need to handle the fact that the prediction for what token t will be is\n",
    "      made after seeing only t - 1 tokens; that is, there is an off-by-one shift needed\n",
    "      between the logits and targets.\n",
    "\n",
    "    Args:\n",
    "      logits: a 2D [batch_size, n_classes] (for classification) or 3D\n",
    "        [batch_size, sequence_length, vocab_size] (for generation) tensor of logits\n",
    "      targets: a 1D [batch_size] (for classification) or 2D [batch_size, sequence_length]\n",
    "        (for generation) tensor of target indices. For the generation case, may contain\n",
    "        -100 in some positions, meaning that the loss for this timestep should be ignored.\n",
    "    \n",
    "    Returns:\n",
    "      A *scalar* representing the average exact-match accuracy over all non-masked batch \n",
    "        elements (and sequence timesteps, if applicable)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    if logits.dim() == 2:\n",
    "        pass\n",
    "    elif logits.dim() == 3:\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(f'Logits should either be 2-dim (for classification) or 3-dim (for generation); got {logits.dim()}')\n",
    "\n",
    "\n",
    "def ft_bert(model, tok, x, y, mode, batch_size=8):\n",
    "    model = copy.deepcopy(model)\n",
    "\n",
    "    if mode.startswith('lora'):\n",
    "        for m in model.transformer.h:\n",
    "            m.mlp.c_fc = LoRAConv1DWrapper(m.mlp.c_fc, int(mode[4:]))\n",
    "            m.mlp.c_proj = LoRAConv1DWrapper(m.mlp.c_proj, int(mode[4:]))\n",
    "\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.Adam(parameters_to_fine_tune(model, mode), lr=1e-4)\n",
    "    all_x = tok(x, return_tensors='pt', padding=True, truncation=True, max_length=100).to(DEVICE)\n",
    "    all_y = torch.tensor(y, device=DEVICE)\n",
    "    pbar = tqdm.tqdm(range(1000))\n",
    "    for step in pbar:\n",
    "        batch = np.random.randint(0, len(x), batch_size)\n",
    "        x_ = tok([x[i] for i in batch], return_tensors='pt', padding=True, truncation=True, max_length=100).to(DEVICE)\n",
    "        y_ = torch.tensor([y[i] for i in batch], device=DEVICE)\n",
    "        logits = model(**x_).logits\n",
    "        loss = get_loss(logits, y_)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if args.debug:\n",
    "            break\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            with torch.inference_mode():\n",
    "                total_acc = get_acc(model(**all_x).logits, all_y)\n",
    "            pbar.set_description(f'Fine-tuning acc: {total_acc:.04f}')\n",
    "            if total_acc > 0.75:\n",
    "                break\n",
    "    return model\n",
    "\n",
    "\n",
    "def tokenize_gpt2_batch(tokenizer, x, y):\n",
    "    \"\"\"\n",
    "    Implement the tokenization step for a batch of examples for GPT-2.\n",
    "\n",
    "    Args:\n",
    "        tokenizer: a GPT2Tokenizer that you can call and receive a dictionary of:\n",
    "          - input_ids: a list (or tensor) of token ids\n",
    "          - attention_mask: a list (or tensor) of 1s and 0s indicating which tokens\n",
    "              are padding (if you requested padding and tensors from the tokenizer)\n",
    "        x: a list of strings, each of which is the input for a single example\n",
    "        y: a list of strings, each of which is a *target* for a single example\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary with the following keys:\n",
    "            - input_ids: a tensor of shape [batch_size, sequence_length] \n",
    "                containing the token ids\n",
    "            - attention_mask: a tensor of shape [batch_size, sequence_length] \n",
    "                containing 1s and 0s indicating which tokens are padding\n",
    "            - labels: a tensor of shape [batch_size, sequence_length] containing\n",
    "                the target token ids, with -100 for non-target tokens (i.e., the\n",
    "                tokens in the input part of each example or padding tokens)\n",
    "        where sequence_length is determined by the (x, y) pair whose tokenized\n",
    "        length is the longest in the batch. The other sequences should be padded to\n",
    "        this length (you can get the tokenizer to handle this padding!).\n",
    "\n",
    "    Example:\n",
    "        >>> x = ['Who is the singer for the band Queen?', 'What is the capital of France?']\n",
    "        >>> y = ['Freddie Mercury', 'Paris']\n",
    "        >>> tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        >>> tokenizer_dict = tokenizer([x_ + y_ for x_, y_ in zip(x, y)], return_tensors='pt', padding=True)\n",
    "        >>> tokenizer_dict['input_ids']\n",
    "        tensor([[ 8241,   318,   262, 14015,   329,   262,  4097,  7542,    30, 30847, 11979, 21673],\n",
    "                [ 2061,   318,   262,  3139,   286,  4881,    30, 40313, 50256, 50256, 50256, 50256]])\n",
    "        >>> tokenizer_dict['attention_mask']\n",
    "        tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "                [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])\n",
    "        >>> tokenizer(x)['input_ids']\n",
    "        [[8241, 318, 262, 14015, 329, 262, 4097, 7542, 30],\n",
    "         [2061, 318, 262, 3139, 286, 4881, 30]]\n",
    "        >>> tokenizer(y)['input_ids']\n",
    "        [[30847, 11979, 21673],\n",
    "         [40313]]\n",
    "\n",
    "        In this case, our labels should look like:\n",
    "        [[-100, -100, -100, -100, -100, -100, -100, -100,   -100,  30847, 11979, 21673],\n",
    "         [-100, -100, -100, -100, -100, -100, -100,  40313, -100, -100,  -100,  -100]]\n",
    "        Note we've replaced padding tokens and the input prefix for each example\n",
    "            with -100, leaving only the tokens in y.\n",
    "\n",
    "        Other note: you can add new keys (such as 'labels') to the dictionary\n",
    "            returned by the tokenizer without creating a new dictionary.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    tokenized_sequences = None\n",
    "    return tokenized_sequences\n",
    "\n",
    "\n",
    "def add_prefixes(x: List[str], y: List[str], dataset: str) -> Tuple[List[str], List[str]]:\n",
    "    input_prefix = '' if utils.is_qa_dataset(dataset) else ''\n",
    "    label_prefix = ' In the' if utils.is_qa_dataset(dataset) else ' TL;DR:'\n",
    "    label_suffix = '.' if utils.is_qa_dataset(dataset) else ''\n",
    "\n",
    "    x = [input_prefix + x_.replace('\\n', ' ') + label_prefix for x_ in x]\n",
    "    y = [' ' + y_.replace('\\n', ' ') + label_suffix for y_ in y]\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def ft_gpt2(model, tok, x, y, mode, dataset, batch_size=8, grad_accum=8):\n",
    "    x, y = add_prefixes(x, y, dataset)\n",
    "\n",
    "    model = copy.deepcopy(model)\n",
    "\n",
    "    if mode.startswith('lora'):\n",
    "        for m in model.transformer.h:\n",
    "            m.mlp.c_fc = LoRAConv1DWrapper(m.mlp.c_fc, int(mode[4:]))\n",
    "            m.mlp.c_proj = LoRAConv1DWrapper(m.mlp.c_proj, int(mode[4:]))\n",
    "            m.attn.c_attn = LoRAConv1DWrapper(m.attn.c_attn, int(mode[4:]))\n",
    "\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.Adam(parameters_to_fine_tune(model, mode), lr=2e-5)\n",
    "    all_both = tokenize_gpt2_batch(tok, x, y)\n",
    "    max_n = len(x) * 10\n",
    "    pbar = tqdm.tqdm(range(max_n))\n",
    "    idxs = []\n",
    "    for step in pbar:\n",
    "        model.train()\n",
    "\n",
    "        if len(idxs) < batch_size // grad_accum:\n",
    "            idxs = list(range(len(x)))\n",
    "            random.shuffle(idxs)\n",
    "        batch_idxs = idxs[:batch_size // grad_accum]\n",
    "        idxs = idxs[batch_size // grad_accum:]\n",
    "\n",
    "        # Outline:\n",
    "        # 1. Sample a random minibatch of examples of size batch_size // grad_accum using the batch_idxs variable\n",
    "        # 2. Tokenize the batch using the tokenize_gpt2_batch function you implemented\n",
    "        # 3. Run the model on the batch, get the logits, and compute the loss using the get_loss function you implemented\n",
    "        #      *NOTE 1* Pass `use_cache=False` when you call model() to avoid a huggingface warning\n",
    "        #      *NOTE 2* You MUST compute the loss using your get_loss function applied to the model_output.logits.\n",
    "        #        Don't use the loss attribute of the model output for training (you will not get credit for this).\n",
    "        #        However, you can use the loss attribute of the model output to test your get_loss function (they should match).\n",
    "        # 4. Backpropagate the loss (divided by the grad_accum parameter)\n",
    "        # 5. Take a step of the optimizer and zero the model gradients ***only every grad_accum steps***\n",
    "        #    Be careful that you don't take a step after the very first backward pass (i.e., when step == 0)\n",
    "        # Note: the ** operator will unpack a dictionary into keyword arguments to a function (such as your model)\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # END YOUR CODE\n",
    "\n",
    "        if step % (grad_accum * 5) == 0:\n",
    "            with torch.inference_mode():\n",
    "                model.eval()\n",
    "                accs = []\n",
    "                for idx in range(len(list(all_both.values())[0])):\n",
    "                    d = {k: v[idx:idx+1] for k, v in all_both.items()}\n",
    "                    acc = get_acc(model(**d).logits, d['labels'])\n",
    "                    accs.append(acc)\n",
    "                total_acc = sum(accs) / len(accs)\n",
    "                pbar.set_description(f'Fine-tuning acc: {total_acc:.04f}')\n",
    "\n",
    "            if total_acc >= utils.early_stop_thresold(dataset):\n",
    "                print('Early stopping!')\n",
    "                break\n",
    "    return model\n",
    "\n",
    "\n",
    "def eval(model, tok, val_data):\n",
    "    x = tok(val_data['x'], return_tensors='pt', padding=True, truncation=True, max_length=100).to(DEVICE)\n",
    "    y = torch.tensor(val_data['y'], device=DEVICE)\n",
    "    with torch.inference_mode():\n",
    "        logits = model(**x).logits\n",
    "    return get_acc(logits, y)\n",
    "\n",
    "\n",
    "def run_ft(models: List[str], datasets: List[str], ks: List[int], modes: List[str], n_val: int = 125):\n",
    "    results = {}\n",
    "    for dataset in datasets:\n",
    "        if args.debug:\n",
    "            n_val = 1   \n",
    "        train, val = utils.get_dataset(dataset, max(ks), n_val=n_val)\n",
    "        for model_name, mode in itertools.product(models, modes):\n",
    "            if dataset == 'amazon':\n",
    "                model, tokenizer = utils.get_model_and_tokenizer(model_name, transformers.AutoModelForSequenceClassification, num_labels=5)\n",
    "            else:\n",
    "                model, tokenizer = utils.get_model_and_tokenizer(model_name, transformers.AutoModelForCausalLM)\n",
    "            stop_tokens = utils.stop_tokens(tokenizer)\n",
    "\n",
    "            for k in ks:\n",
    "                print(f'Fine-tuning {model_name} on {dataset} with k={k} and mode={mode}')\n",
    "                for repeat in range(args.repeats):\n",
    "                    if repeat > 0:\n",
    "                        print(f'Beginning repeat #{repeat}')\n",
    "                    if dataset == 'amazon':\n",
    "                        fine_tuned = ft_bert(model, tokenizer, train['x'][:k*5], train['y'][:k*5], mode)\n",
    "                        val_acc = eval(fine_tuned, tokenizer, val)\n",
    "                        results['_'.join([model_name, dataset, str(k), mode])] = val_acc\n",
    "                    else:\n",
    "                        if k > 0:\n",
    "                            fine_tuned = ft_gpt2(model, tokenizer, train['x'][:k], train['simple_y'][:k], mode, dataset)\n",
    "                        else:\n",
    "                            fine_tuned = copy.deepcopy(model)\n",
    "                            fine_tuned.to(DEVICE)\n",
    "\n",
    "                        fine_tuned.eval()\n",
    "                        targets = []\n",
    "                        predictions = []\n",
    "                        pbar = tqdm.tqdm(list(range(min(n_val, len(val['x'])))))\n",
    "\n",
    "                        for row in pbar:\n",
    "                            test_input = val['x'][row]\n",
    "                            targets.append(val['y'][row])\n",
    "                            max_tokens = utils.max_sampled_tokens_for_dataset(dataset)\n",
    "                            prompt_mode = 'qa' if utils.is_qa_dataset(dataset) else 'tldr'\n",
    "                            prompt = icl.get_icl_prompts([], [], test_input, prompt_mode=prompt_mode)\n",
    "                            input_ids = tokenizer(prompt, return_tensors='pt').input_ids.to(DEVICE)\n",
    "                            sampled_tokens = icl.do_sample(fine_tuned, input_ids, stop_tokens, max_tokens)\n",
    "                            decoded = tokenizer.decode(sampled_tokens).strip()\n",
    "                            predictions.append(decoded)\n",
    "                            metric = icl.get_performance_metric(predictions, targets, utils.metric_for_dataset(dataset))\n",
    "                            pbar.set_description(f'Eval: {metric:.04f}')\n",
    "                        results['_'.join([model_name, dataset, str(k), mode])] = metric\n",
    "\n",
    "                    print(results)\n",
    "                    question = 'ft'\n",
    "                    if not os.path.exists(f'results/{question}'):\n",
    "                        os.makedirs(f'results/{question}')\n",
    "\n",
    "                    for k_, v in results.items():\n",
    "                        with open(f'results/{question}/{k_}.json', 'w') as f:\n",
    "                            json.dump({'metric': v}, f)\n",
    "                    results = {}\n",
    "\n",
    "\n",
    "def plot(models, datasets, ks, modes):\n",
    "    data = defaultdict(lambda: defaultdict(list))\n",
    "    question = 'ft'\n",
    "\n",
    "    x_vals = set()\n",
    "    for dataset in datasets:\n",
    "        for model, mode in itertools.product(models, modes):\n",
    "            for k in ks:\n",
    "                fn = '_'.join([model, dataset, str(k), mode])\n",
    "                id_ = '_'.join([model, dataset, mode])\n",
    "                with open(f'results/{question}/{fn}.json', 'r') as f:\n",
    "                    score = json.load(f)['metric']\n",
    "                    data[id_]['x'].append(k)\n",
    "                    x_vals.add(k)\n",
    "                    data[id_]['y'].append(score)\n",
    "\n",
    "        for k, v in data.items():\n",
    "            plt.plot(v['x'], v['y'], label=k)\n",
    "\n",
    "    if max(x_vals) > 4:\n",
    "        plt.xscale('symlog')\n",
    "    ax = plt.gca()\n",
    "    ax.xaxis.set_major_formatter(mticker.ScalarFormatter())\n",
    "    ax.xaxis.set_ticks(sorted(x_vals))\n",
    "    plt.legend()\n",
    "    plt.title(' & '.join(datasets))\n",
    "    plt.ylabel('/'.join([utils.metric_for_dataset(dataset) for dataset in datasets]))\n",
    "    plt.xlabel('Number of support examples')\n",
    "    plt.show()\n",
    "\n",
    "def run():\n",
    "    ks = [int(k) for k in args.k.split(',')]\n",
    "    if args.task == 'ft':\n",
    "        run_ft(args.model.split(','), args.dataset.split(','), ks, args.mode.split(','))\n",
    "    elif args.task == 'plot':\n",
    "        plot(args.model.split(','), args.dataset.split(','), ks, args.mode.split(','))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
