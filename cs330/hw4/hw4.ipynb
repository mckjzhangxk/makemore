{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "import functools\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from transformers import AutoModel,AutoTokenizer,AutoModelForCausalLM\n",
    "torch.set_printoptions(linewidth=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained('gpt2-medium')\n",
    "model=AutoModelForCausalLM.from_pretrained('gpt2-medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dummy(nn.Module):\n",
    "    def __init__(self, conv1dmodule: nn.Module,lora_rank: int) :\n",
    "        super().__init__()\n",
    "        self.base_module = conv1dmodule\n",
    "\n",
    "        \n",
    "        size=conv1dmodule.get_parameter(\"weight\").shape\n",
    "       \n",
    "        self.A=nn.Parameter(torch.randn(size[0],lora_rank),True)\n",
    "        self.B=nn.Parameter(torch.zeros(lora_rank,size[1]),True)\n",
    "    def forward(self,x):\n",
    "\n",
    "        y=self.base_module(x)\n",
    "        z=x@self.A@self.B\n",
    "        return y+z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 4096]) torch.Size([4096, 1024])\n",
      "torch.Size([1024, 3072]) torch.Size([1024, 1024])\n"
     ]
    }
   ],
   "source": [
    "for i,m in enumerate(model.transformer.h):\n",
    "    # print(i)\n",
    "    print(m.mlp.c_fc.get_parameter(\"weight\").shape,m.mlp.c_proj.get_parameter(\"weight\").shape)\n",
    "    print(m.attn.c_attn.get_parameter(\"weight\").shape,m.attn.c_proj.get_parameter(\"weight\").shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in model.transformer.h:\n",
    "    m.attn.c_attn=Dummy(m.attn.c_attn,2)\n",
    "    m.mlp.c_fc=Dummy(m.mlp.c_fc,2)\n",
    "    m.mlp.c_proj=Dummy(m.mlp.c_proj,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token=tokenizer.eos_token\n",
    "inp=tokenizer(['hello','Good bye'],['world',' jone'],padding=True,return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 50257])"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**inp,use_cache=False).logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 1024)\n",
       "    (wpe): Embedding(1024, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Dummy(\n",
       "            (base_module): Conv1D()\n",
       "          )\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Dummy(\n",
       "            (base_module): Conv1D()\n",
       "          )\n",
       "          (c_proj): Dummy(\n",
       "            (base_module): Conv1D()\n",
       "          )\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters=[]\n",
    "for m in model.transformer.h:\n",
    "    parameters.extend([m.attn.c_attn.A,m.attn.c_attn.B])\n",
    "    parameters.extend([m.mlp.c_fc.A,m.mlp.c_fc.B])\n",
    "    parameters.extend([m.mlp.c_proj.A,m.mlp.c_proj.B])\n",
    "    \n",
    "    # m.attn.c_attn=Dummy(m.attn.c_attn,2)\n",
    "    # m.mlp.c_fc=Dummy(m.mlp.c_fc,2)\n",
    "    # m.mlp.c_proj=Dummy(m.mlp.c_proj,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters=[]\n",
    "for m in model.modules():\n",
    "    if isinstance(m,Dummy):\n",
    "        parameters.extend([m.A,m.B])\n",
    "# parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=['Who is the singer for the band Queen?', 'What is the capital of France?', 'What is the capital of France?']\n",
    "y=['Freddie Mercury', 'Paris','Freddie Mercury']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=tokenizer(x,y,padding=True,return_tensors='pt')\n",
    "labels=inputs.input_ids.clone()\n",
    "\n",
    "labels[inputs.attention_mask==0]=-100\n",
    "\n",
    "x_len=tokenizer(x,padding=True,return_tensors='pt').attention_mask.sum(1)\n",
    "\n",
    "\n",
    "for i in range(len(x_len)):\n",
    "    labels[i,0:x_len[i]]=-100    \n",
    "# tokenizer.decode(ids[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 8241,   318,   262, 14015,   329,   262,  4097,  7542,    30, 30847, 11979, 21673],\n",
      "        [ 2061,   318,   262,  3139,   286,  4881,    30, 40313, 50256, 50256, 50256, 50256],\n",
      "        [ 2061,   318,   262,  3139,   286,  4881,    30, 30847, 11979, 21673, 50256, 50256]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])\n",
      "tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 30847, 11979, 21673],\n",
      "        [ -100,  -100,  -100,  -100,  -100,  -100,  -100, 40313,  -100,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  -100,  -100,  -100,  -100, 30847, 11979, 21673,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "print(inputs.input_ids)\n",
    "print(inputs.attention_mask)\n",
    "print(labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        >>> x = ['Who is the singer for the band Queen?', 'What is the capital of France?']\n",
    "        >>> y = ['Freddie Mercury', 'Paris']\n",
    "        >>> tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        >>> tokenizer_dict = tokenizer([x_ + y_ for x_, y_ in zip(x, y)], return_tensors='pt', padding=True)\n",
    "        >>> tokenizer_dict['input_ids']\n",
    "        tensor([[ 8241,   318,   262, 14015,   329,   262,  4097,  7542,    30, 30847, 11979, 21673],\n",
    "                [ 2061,   318,   262,  3139,   286,  4881,    30, 40313, 50256, 50256, 50256, 50256]])\n",
    "        >>> tokenizer_dict['attention_mask']\n",
    "        tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "                [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])\n",
    "        >>> tokenizer(x)['input_ids']\n",
    "        [[8241, 318, 262, 14015, 329, 262, 4097, 7542, 30],\n",
    "         [2061, 318, 262, 3139, 286, 4881, 30]]\n",
    "        >>> tokenizer(y)['input_ids']\n",
    "        [[30847, 11979, 21673],\n",
    "         [40313]]\n",
    "\n",
    "        In this case, our labels should look like:\n",
    "        [[-100, -100, -100, -100, -100, -100, -100, -100,   -100,  30847, 11979, 21673],\n",
    "         [-100, -100, -100, -100, -100, -100, -100,  40313, -100, -100,  -100,  -100]]\n",
    "        Note we've replaced padding tokens and the input prefix for each example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 1024)\n",
       "    (wpe): Embedding(1024, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Dummy(\n",
       "            (base_module): Conv1D()\n",
       "          )\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Dummy(\n",
       "            (base_module): Conv1D()\n",
       "          )\n",
       "          (c_proj): Dummy(\n",
       "            (base_module): Conv1D()\n",
       "          )\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp=dict(input_ids=inputs.input_ids,attention_mask=inputs.attention_mask,labels=labels)\n",
    "inputs['labels']=labels\n",
    "inputs=inputs.to('cuda')\n",
    "\n",
    "out=model(**inputs,use_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2857, device='cuda:0')"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits=out.logits[:,:-1,:]\n",
    "target=inputs.labels[:,1:]\n",
    "B,T=target.shape\n",
    "mask=target!=-100\n",
    "(logits[mask].argmax(-1)==target[mask]).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8241,   318,   262, 14015,   329,   262,  4097,  7542,    30, 30847, 11979, 21673],\n",
       "        [ 2061,   318,   262,  3139,   286,  4881,    30, 40313, 50256, 50256, 50256, 50256],\n",
       "        [ 2061,   318,   262,  3139,   286,  4881,    30, 30847, 11979, 21673, 50256, 50256]], device='cuda:0')"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(inputs.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 8241,   318,   262, 14015,   329,   262,  4097,  7542,    30, 30847, 11979, 21673]], device='cuda:0'),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0'),\n",
       " 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 30847, 11979, 21673]], device='cuda:0')}"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k:v[0:1] for k,v in inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('input_ids', tensor([[ 8241,   318,   262, 14015,   329,   262,  4097,  7542,    30, 30847, 11979, 21673],\n",
       "        [ 2061,   318,   262,  3139,   286,  4881,    30, 40313, 50256, 50256, 50256, 50256],\n",
       "        [ 2061,   318,   262,  3139,   286,  4881,    30, 30847, 11979, 21673, 50256, 50256]], device='cuda:0')), ('attention_mask', tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]], device='cuda:0')), ('labels', tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 30847, 11979, 21673],\n",
       "        [ -100,  -100,  -100,  -100,  -100,  -100,  -100, 40313,  -100,  -100,  -100,  -100],\n",
       "        [ -100,  -100,  -100,  -100,  -100,  -100,  -100, 30847, 11979, 21673,  -100,  -100]], device='cuda:0'))])"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 1024)\n",
       "    (wpe): Embedding(1024, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Dummy(\n",
       "            (base_module): Conv1D()\n",
       "          )\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Dummy(\n",
       "            (base_module): Conv1D()\n",
       "          )\n",
       "          (c_proj): Dummy(\n",
       "            (base_module): Conv1D()\n",
       "          )\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
