{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt # for making figures\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "oYR8u0D27V2Y"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FU1BBXQ267hm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c853f7f-2f90-4ea9-9616-3f180229dda3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-25 04:01:34--  https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228145 (223K) [text/plain]\n",
            "Saving to: ‘names.txt’\n",
            "\n",
            "names.txt           100%[===================>] 222.80K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2023-03-25 04:01:34 (77.9 MB/s) - ‘names.txt’ saved [228145/228145]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# download the names.txt file from github\n",
        "!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class CausalConv1D(nn.Module):\n",
        "    def __init__(self,inChan,outChan,dilation=1,nonlinearity='linear'):\n",
        "        super().__init__()\n",
        "        K=2\n",
        "        # self.receive_field=K+(K-1)*(dilation-1)\n",
        "\n",
        "        self.conv=nn.Conv1d(inChan,outChan,K,dilation=dilation,bias=False)\n",
        "\n",
        "        nn.init.kaiming_normal_(self.conv.weight,  nonlinearity=nonlinearity)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x=F.pad(x, (self.receive_field-1,0),mode='constant', value=0)\n",
        "        self.out=self.conv(x)\n",
        "\n",
        "        return  self.out\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self,inChan,dilation=1):\n",
        "        super(Block, self).__init__()\n",
        "\n",
        "        self.causul_conv=CausalConv1D(inChan,inChan,dilation,nonlinearity='tanh')  #for tanh\n",
        "        self.causul_conv2 = CausalConv1D(inChan, inChan, dilation) #for sigma\n",
        "\n",
        "        self.tanh=nn.Tanh()\n",
        "        self.sigmoid=nn.Sigmoid()\n",
        "\n",
        "\n",
        "        self.output_conv=nn.Conv1d(inChan,inChan,1)\n",
        "        self.skip_conv = nn.Conv1d(inChan, inChan, 1)\n",
        "\n",
        "\n",
        "    def forward(self, x,debug=False):\n",
        "        h=self.causul_conv(x)\n",
        "        h1=self.causul_conv2(x)\n",
        "\n",
        "        th=self.tanh(h)\n",
        "        sh=self.sigmoid(h1)\n",
        "        w=th*sh\n",
        "\n",
        "        o=self.output_conv(w)+x[:,:,-h.shape[2]:]\n",
        "        s=self.skip_conv(w)\n",
        "        return o,s\n",
        "\n",
        "\n",
        "\n",
        "class WaveNet(nn.Module):\n",
        "    def __init__(self,classes,resChan,stacks,layers):\n",
        "        super(WaveNet, self).__init__()\n",
        "\n",
        "        dilations=[2**l for l in range(layers)]*stacks\n",
        "\n",
        "        self.receive_filed=1\n",
        "        for i,d in enumerate(dilations):\n",
        "            self.receive_filed+=d\n",
        "\n",
        "\n",
        "        self.emb_layer=nn.Embedding(classes,resChan)\n",
        "        # bug,没有加这个ModuleList,这些block对于的参数网络\n",
        "        self.layers=nn.ModuleList([Block(resChan,d) for d in dilations])\n",
        "\n",
        "\n",
        "        self.post_layer=nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(resChan,resChan,1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(resChan, classes, 1),\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        # x.shape =(B,T,classes)\n",
        "        B,T=x.shape\n",
        "        remain=T-self.receive_filed+1\n",
        "\n",
        "        emb=self.emb_layer(x).transpose(1,2)\n",
        "\n",
        "        h=emb\n",
        "\n",
        "        output=0\n",
        "        for layer in self.layers:\n",
        "            h,s=layer(h)\n",
        "            output+=s[:,:,-remain:]\n",
        "\n",
        "        self.skipout=output\n",
        "        # (B,classes,T)\n",
        "        logit=self.post_layer(output)\n",
        "\n",
        "        return torch.squeeze(logit)\n"
      ],
      "metadata": {
        "id": "tGBwpYCr7cCO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataSet():\n",
        "    def __init__(self):\n",
        "        self.words = open('names.txt').read().splitlines()\n",
        "        random.shuffle(self.words)\n",
        "\n",
        "        self.chs = sorted(set(''.join(self.words)))\n",
        "        self.stoi = {c: i + 1 for i, c in enumerate(self.chs)}\n",
        "        self.stoi['.'] = 0\n",
        "        self.itos = {i: c for c, i in self.stoi.items()}\n",
        "        self.VSIZE=len(self.itos)\n",
        "    def index2Str(self,l):\n",
        "        return \"\".join([self.itos[id] for id in l])\n",
        "    def _build_dataset(self,words,contextSize):\n",
        "        Xs,Ys=[],[]\n",
        "        for word in words:\n",
        "            context = [0] * contextSize\n",
        "            for ch in word+\".\":\n",
        "                idx=self.stoi[ch]\n",
        "                Xs.append(context)\n",
        "                Ys.append(idx)\n",
        "                context=context[1:]+[idx]\n",
        "        return torch.tensor(Xs),torch.tensor(Ys)\n",
        "\n",
        "    def getData(self,tag,contextSize):\n",
        "        n1 = int(len(self.words) * 0.8)\n",
        "        n2 = int(len(self.words) * 0.9)\n",
        "\n",
        "        if tag=='train':\n",
        "            X,Y=self._build_dataset(self.words[0:n1],contextSize)\n",
        "        elif tag=='val':\n",
        "            X, Y = self._build_dataset(self.words[n1:n2],contextSize)\n",
        "        elif tag=='test':\n",
        "            X, Y = self._build_dataset(self.words[n2:],contextSize)\n",
        "\n",
        "        print(f\"{tag}: X: {X.shape}, Y: {Y.shape}\")\n",
        "\n",
        "        return X,Y"
      ],
      "metadata": {
        "id": "4FIQ4krA7nv9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hparams={\n",
        "        \"contextSize\":8,\n",
        "        \"embSize\":16,\n",
        "        \"hiddenSize\":32,\n",
        "        \"steps\":20000,\n",
        "        \"batch_size\":32,\n",
        "        \"Wgain\":5/3,\n",
        "        \"softmax_gain\":0.01\n",
        "}\n",
        "\n",
        "random.seed(42)\n",
        "torch.manual_seed(2147483647)\n",
        "\n",
        "\n",
        "ds=DataSet()\n",
        "Xtr,Ytr=ds.getData('train',hparams[\"contextSize\"])\n",
        "Xdev,Ydev=ds.getData('val',hparams[\"contextSize\"])\n",
        "Xtest,Ytest=ds.getData('test',hparams[\"contextSize\"])\n",
        "\n",
        "\n",
        "embSize=hparams[\"embSize\"]\n",
        "hiddenSize=hparams[\"hiddenSize\"]\n",
        "Wgain=hparams[\"Wgain\"]\n",
        "softmax_gain=hparams[\"softmax_gain\"]\n",
        "\n",
        "model=WaveNet(ds.VSIZE,hiddenSize,1,3)\n",
        "\n",
        "def zeros_grad(model):\n",
        "    for p in model.parameters():\n",
        "       p.grad=None\n",
        "model.load_state_dict(torch.load(\"wavenet_model.pth\"),strict=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpJPek9-8Dv4",
        "outputId": "beb55b25-2f99-4228-9bbc-e1911dffdaaf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: X: torch.Size([182625, 8]), Y: torch.Size([182625])\n",
            "val: X: torch.Size([22655, 8]), Y: torch.Size([22655])\n",
            "test: X: torch.Size([22866, 8]), Y: torch.Size([22866])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ncount=sum([p.nelement() for p in model.parameters()])\n",
        "print(f\"# {ncount}\")\n",
        "\n",
        "print(\"--------------training---------------\")\n",
        "print(f\"hparams:{hparams}\")\n",
        "print()\n",
        "\n",
        "model.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUbEfpCG8dxQ",
        "outputId": "c5729d91-2ad8-4347-b24c-de6ed0e9caa6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# 21435\n",
            "--------------training---------------\n",
            "hparams:{'contextSize': 8, 'embSize': 16, 'hiddenSize': 32, 'steps': 20000, 'batch_size': 32, 'Wgain': 1.6666666666666667, 'softmax_gain': 0.01}\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WaveNet(\n",
              "  (emb_layer): Embedding(27, 32)\n",
              "  (layers): ModuleList(\n",
              "    (0): Block(\n",
              "      (causul_conv): CausalConv1D(\n",
              "        (conv): Conv1d(32, 32, kernel_size=(2,), stride=(1,), bias=False)\n",
              "      )\n",
              "      (causul_conv2): CausalConv1D(\n",
              "        (conv): Conv1d(32, 32, kernel_size=(2,), stride=(1,), bias=False)\n",
              "      )\n",
              "      (tanh): Tanh()\n",
              "      (sigmoid): Sigmoid()\n",
              "      (output_conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
              "      (skip_conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
              "    )\n",
              "    (1): Block(\n",
              "      (causul_conv): CausalConv1D(\n",
              "        (conv): Conv1d(32, 32, kernel_size=(2,), stride=(1,), dilation=(2,), bias=False)\n",
              "      )\n",
              "      (causul_conv2): CausalConv1D(\n",
              "        (conv): Conv1d(32, 32, kernel_size=(2,), stride=(1,), dilation=(2,), bias=False)\n",
              "      )\n",
              "      (tanh): Tanh()\n",
              "      (sigmoid): Sigmoid()\n",
              "      (output_conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
              "      (skip_conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
              "    )\n",
              "    (2): Block(\n",
              "      (causul_conv): CausalConv1D(\n",
              "        (conv): Conv1d(32, 32, kernel_size=(2,), stride=(1,), dilation=(4,), bias=False)\n",
              "      )\n",
              "      (causul_conv2): CausalConv1D(\n",
              "        (conv): Conv1d(32, 32, kernel_size=(2,), stride=(1,), dilation=(4,), bias=False)\n",
              "      )\n",
              "      (tanh): Tanh()\n",
              "      (sigmoid): Sigmoid()\n",
              "      (output_conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
              "      (skip_conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
              "    )\n",
              "  )\n",
              "  (post_layer): Sequential(\n",
              "    (0): ReLU()\n",
              "    (1): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
              "    (2): ReLU()\n",
              "    (3): Conv1d(32, 27, kernel_size=(1,), stride=(1,))\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model=model.to(device)\n",
        "@torch.no_grad()\n",
        "def splitLoss(split):\n",
        "    d={\n",
        "        'train':(Xtr,Ytr),\n",
        "        'dev':(Xdev,Ydev),\n",
        "        'tetst':(Xtest,Ytest)\n",
        "    }\n",
        "    xs,ys=d[split]\n",
        "    if split==\"train\":\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "    loss=F.cross_entropy(model(xs.to(device)),ys.to(device)).item()\n",
        "    return loss"
      ],
      "metadata": {
        "id": "6JoMRjmA9qUN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre=0\n",
        "for step in range(200000):\n",
        "    idx=torch.randint(0,Xtr.shape[0],(hparams[\"batch_size\"],),generator=None)\n",
        "    xs_batch=Xtr[idx].to(device)\n",
        "    ys_batch=Ytr[idx].to(device)\n",
        "\n",
        "\n",
        "    logit=model(xs_batch)\n",
        "    nll=F.cross_entropy(logit,ys_batch)\n",
        "\n",
        "    # model.zero_grads()\n",
        "    zeros_grad(model)\n",
        "    model.skipout.retain_grad()\n",
        "    nll.backward()\n",
        "\n",
        "    lr = 0.1 if step < 100000 else 0.01\n",
        "    for p in model.parameters():\n",
        "       if p.grad == None:\n",
        "           continue\n",
        "       p.data-=lr*p.grad\n",
        "    # print(nll.item(),(model.skipout.grad/model.skipout).abs().mean())\n",
        "    # print(nll.item(),(pre-model.skipout).abs().mean())\n",
        "    pre=model.skipout\n",
        "    if( step%2000==0):\n",
        "        train_loss=nll.item()\n",
        "        dev_loss=splitLoss('dev')\n",
        "        print(f'{step:7d}/{hparams[\"steps\"]:7d} train_loss {train_loss:.4f},dev_loss {dev_loss:.4f}')\n",
        "        model.train()\n",
        "print(\"--------------training end---------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTD5BTwu8tR0",
        "outputId": "e5a2b940-6197-4f39-8e2d-06df31d6a899"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 194000/  20000 train_loss 1.9492,dev_loss 2.1096\n",
            " 196000/  20000 train_loss 1.9178,dev_loss 2.1104\n",
            " 198000/  20000 train_loss 2.2214,dev_loss 2.1103\n",
            "--------------training end---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_train_loss=splitLoss('train')\n",
        "final_dev_loss=splitLoss('dev')\n",
        "print(f\"train loss {final_train_loss:.4f}, dev loss {final_dev_loss:.4f}\")\n",
        "torch.save(model.state_dict(), \"wavenet_model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jcrEGBV-B0-",
        "outputId": "a9c413ba-fbc2-4eb9-df2a-8d54d5c69360"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss 2.0381, dev loss 2.1107\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1tVuIblV-uJ5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}