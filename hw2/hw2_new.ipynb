{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c0219b",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import google_drive_downloader as gdd\n",
    "import imageio.v2 as imageio\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils import tensorboard\n",
    "from torch.utils.data import dataloader,dataset,sampler\n",
    "from torch import autograd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a615c5",
   "metadata": {},
   "source": [
    "### 基础工具类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee564fee",
   "metadata": {
    "code_folding": [
     0,
     18,
     43
    ],
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def score(logits, labels):\n",
    "    \"\"\"Returns the mean accuracy of a model's predictions on a set of examples.\n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensor): model predicted logits\n",
    "            shape (examples, classes)\n",
    "        labels (torch.Tensor): classification labels from 0 to num_classes - 1\n",
    "            shape (examples,)\n",
    "    \"\"\"\n",
    "\n",
    "    assert logits.dim() == 2\n",
    "    assert labels.dim() == 1\n",
    "    assert logits.shape[0] == labels.shape[0]\n",
    "    y = torch.argmax(logits, dim=-1) == labels\n",
    "    y = y.type(torch.float)\n",
    "    return torch.mean(y).item()\n",
    "\n",
    "#@title show_data(images,labels)\n",
    "def show_data(images,labels,N,title=''):\n",
    "\n",
    "  '''\n",
    "    images:(N*K,1,28,28)\n",
    "    labels:(N*K,)\n",
    "    每一行，隔K个表示一个类别 \n",
    "  '''\n",
    "  images=images.view(N,-1,1,28,28)\n",
    "  labels=labels.view(N,-1)\n",
    "  N,K=labels.shape[:2]\n",
    "  \n",
    "  # plt.figure(figsize=(18,18))\n",
    "  figure,axes=plt.subplots(N,K,layout='constrained')\n",
    "  for n in range(N):    \n",
    "    for k in range(K):\n",
    "      im,lb=images[n,k],labels[n,k]\n",
    "      if K!=1:\n",
    "        axes[n,k].imshow(im[0],cmap=\"gray\")\n",
    "        axes[n,k].axis('off')\n",
    "        axes[n,k].set_title(f\"C: {lb.item()}\")\n",
    "      else:\n",
    "        axes[n].imshow(im[0],cmap=\"gray\")\n",
    "        axes[n].axis('off')\n",
    "        axes[n].set_title(f\"{lb.item()}\")\n",
    "        \n",
    "def getConfigObject(config):\n",
    "    class MyClass():pass\n",
    "    my_instance = MyClass()\n",
    "    for key, value in config.items():\n",
    "        setattr(my_instance, key, value)\n",
    "    return my_instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c371fc4",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "### 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d2781c",
   "metadata": {
    "code_folding": [
     10,
     26,
     133,
     166,
     177
    ],
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Dataloading for Omniglot.\"\"\"\n",
    "NUM_TRAIN_CLASSES = 1100\n",
    "NUM_VAL_CLASSES = 100\n",
    "NUM_TEST_CLASSES = 423\n",
    "NUM_SAMPLES_PER_CLASS = 20\n",
    "\n",
    "SEED_CLASS=None # 每次取出的task对于的类别确定\n",
    "SEED_IMAGE=None #如果是相同的class,每次取出的图片确定\n",
    "\n",
    "def load_image(file_path):\n",
    "    \"\"\"Loads and transforms an Omniglot image.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): file path of image\n",
    "\n",
    "    Returns:\n",
    "        a Tensor containing image data\n",
    "            shape (1, 28, 28)\n",
    "    \"\"\"\n",
    "    x = imageio.imread(file_path)\n",
    "    x = torch.tensor(x, dtype=torch.float32).reshape([1, 28, 28])\n",
    "    x = x / 255.0\n",
    "    return 1 - x\n",
    "\n",
    "\n",
    "class OmniglotDataset(dataset.Dataset):\n",
    "    \"\"\"Omniglot dataset for meta-learning.\n",
    "\n",
    "    Each element of the dataset is a task. A task is specified with a key,\n",
    "    which is a tuple of class indices (no particular order). The corresponding\n",
    "    value is the instantiated task, which consists of sampled (image, label)\n",
    "    pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    _BASE_PATH = './omniglot_resized'\n",
    "    _GDD_FILE_ID = '1iaSFXIYC3AB8q9K_M-oVMa4pmB7yKMtI'\n",
    "\n",
    "    def __init__(self, num_support, num_query):\n",
    "        \"\"\"Inits OmniglotDataset.\n",
    "\n",
    "        Args:\n",
    "            num_support (int): number of support examples per class\n",
    "            num_query (int): number of query examples per class\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        # if necessary, download the Omniglot dataset\n",
    "        if not os.path.isdir(self._BASE_PATH):\n",
    "            gdd.GoogleDriveDownloader.download_file_from_google_drive(\n",
    "                file_id=self._GDD_FILE_ID,\n",
    "                dest_path=f'{self._BASE_PATH}.zip',\n",
    "                unzip=True\n",
    "            )\n",
    "\n",
    "        # get all character folders\n",
    "        self._character_folders = glob.glob(\n",
    "            os.path.join(self._BASE_PATH, '*/*/'))\n",
    "        assert len(self._character_folders) == (\n",
    "            NUM_TRAIN_CLASSES + NUM_VAL_CLASSES + NUM_TEST_CLASSES\n",
    "        )\n",
    "\n",
    "        # shuffle characters\n",
    "        np.random.default_rng(0).shuffle(self._character_folders)\n",
    "\n",
    "        # check problem arguments\n",
    "        assert num_support + num_query <= NUM_SAMPLES_PER_CLASS\n",
    "        self._num_support = num_support\n",
    "        self._num_query = num_query\n",
    "\n",
    "    def __getitem__(self, class_idxs):\n",
    "        \"\"\"Constructs a task.\n",
    "        Data for each class is sampled uniformly at random without replacement.\n",
    "        The ordering of the labels corresponds to that of class_idxs.\n",
    "\n",
    "        Args:\n",
    "            class_idxs (tuple[int]): class indices that comprise the task\n",
    "\n",
    "        Returns:\n",
    "            images_support (Tensor): task support images\n",
    "                shape (num_way * num_support, channels, height, width)\n",
    "            labels_support (Tensor): task support labels\n",
    "                shape (num_way * num_support,)\n",
    "            images_query (Tensor): task query images\n",
    "                shape (num_way * num_query, channels, height, width)\n",
    "            labels_query (Tensor): task query labels\n",
    "                shape (num_way * num_query,)\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "            采样N个类别，每个类别的采样由class_idxs指定\n",
    "            \n",
    "            对于每个类别，分成support和query数据集\n",
    "                support set：包含 num_support个 该类别的图片/label\n",
    "                query set：包含 num_query个 该类别的图片/label\n",
    "            \n",
    "            最终把support数据集的图片 按照类别的顺序罗列到一起，返回\n",
    "                images_support: (N*num_support, 1,28,28)的图片\n",
    "                images_labels: (N*num_support,) 的标注\n",
    "            把query数据集的图片 按照类别的顺序罗列到一起，返回\n",
    "                images_query: (N*num_query, 1,28,28)的图片\n",
    "                images_query: (N*num_query,) 的标注\n",
    "        \"\"\"\n",
    "        images_support, images_query = [], []\n",
    "        labels_support, labels_query = [], []\n",
    "\n",
    "        for label, class_idx in enumerate(class_idxs):\n",
    "            # get a class's examples and sample from them\n",
    "            all_file_paths = glob.glob(\n",
    "                os.path.join(self._character_folders[class_idx], '*.png')\n",
    "            )\n",
    "            sampled_file_paths = np.random.default_rng(SEED_IMAGE).choice(\n",
    "                all_file_paths,\n",
    "                size=self._num_support + self._num_query,\n",
    "                replace=False\n",
    "            )\n",
    "            images = [load_image(file_path) for file_path in sampled_file_paths]\n",
    "\n",
    "            # split sampled examples into support and query\n",
    "            images_support.extend(images[:self._num_support])\n",
    "            images_query.extend(images[self._num_support:])\n",
    "            labels_support.extend([label] * self._num_support)\n",
    "            labels_query.extend([label] * self._num_query)\n",
    "\n",
    "        # aggregate into tensors\n",
    "        images_support = torch.stack(images_support)  # shape (N*S, C, H, W)\n",
    "        labels_support = torch.tensor(labels_support)  # shape (N*S)\n",
    "        images_query = torch.stack(images_query)\n",
    "        labels_query = torch.tensor(labels_query)\n",
    "\n",
    "        return images_support, labels_support, images_query, labels_query\n",
    "\n",
    "\n",
    "class OmniglotSampler(sampler.Sampler):\n",
    "    \"\"\"Samples task specification keys for an OmniglotDataset.\"\"\"\n",
    "\n",
    "    def __init__(self, split_idxs, num_way, num_tasks):\n",
    "        \"\"\"Inits OmniglotSampler.\n",
    "\n",
    "        Args:\n",
    "            split_idxs (range): indices that comprise the\n",
    "                training/validation/test split\n",
    "            num_way (int): number of classes per task\n",
    "            num_tasks (int): number of tasks to sample\n",
    "        \"\"\"\n",
    "        super().__init__(None)\n",
    "        self._split_idxs = split_idxs\n",
    "        self._num_way = num_way\n",
    "        self._num_tasks = num_tasks\n",
    "\n",
    "    def __iter__(self):\n",
    "        # 1.返回一个迭代器\n",
    "        # 2.每次调用迭代器，从splits_idxs 返回 N个索引, key=[i1,i2,...iN],用于从数据集 取出一组task数据\n",
    "        # 3.此迭代器返回 _num_tasks 个数据，表示训练多少次task\n",
    "        return (\n",
    "            np.random.default_rng(SEED_CLASS).choice(\n",
    "                self._split_idxs,\n",
    "                size=self._num_way,\n",
    "                replace=False\n",
    "            ) for _ in range(self._num_tasks)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._num_tasks\n",
    "\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_omniglot_dataloader(\n",
    "        split,\n",
    "        batch_size,\n",
    "        num_way,\n",
    "        num_support,\n",
    "        num_query,\n",
    "        num_tasks_per_epoch\n",
    "):\n",
    "    \"\"\"Returns a dataloader.DataLoader for Omniglot.\n",
    "\n",
    "    Args:\n",
    "        split (str): one of 'train', 'val', 'test'\n",
    "        batch_size (int): number of tasks per batch\n",
    "        num_way (int): number of classes per task\n",
    "        num_support (int): number of support examples per class\n",
    "        num_query (int): number of query examples per class\n",
    "        num_tasks_per_epoch (int): number of tasks before DataLoader is\n",
    "            exhausted\n",
    "    \"\"\"\n",
    "\n",
    "    if split == 'train':\n",
    "        split_idxs = range(NUM_TRAIN_CLASSES)\n",
    "    elif split == 'val':\n",
    "        split_idxs = range(\n",
    "            NUM_TRAIN_CLASSES,\n",
    "            NUM_TRAIN_CLASSES + NUM_VAL_CLASSES\n",
    "        )\n",
    "    elif split == 'test':\n",
    "        split_idxs = range(\n",
    "            NUM_TRAIN_CLASSES + NUM_VAL_CLASSES,\n",
    "            NUM_TRAIN_CLASSES + NUM_VAL_CLASSES + NUM_TEST_CLASSES\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    return dataloader.DataLoader(\n",
    "        dataset=OmniglotDataset(num_support, num_query),\n",
    "        batch_size=batch_size,\n",
    "        sampler=OmniglotSampler(split_idxs, num_way, num_tasks_per_epoch),\n",
    "        num_workers=0,\n",
    "        collate_fn=identity,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        drop_last=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0cee9b",
   "metadata": {
    "code_folding": [],
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# # # 浏览数据\n",
    "# 一个 dataset，每次返回support和query数据集，每个数据集有N个类别\n",
    "#每个 类别分别有 num_support/num_query 个样本。\n",
    "\n",
    "SEED_IMAGE=None #如果是相同的class,每次取出的图片确定\n",
    "num_support,num_query=4,2\n",
    "ds=OmniglotDataset(num_support, num_query)\n",
    "\n",
    "num_way=5\n",
    "# classids=[5,5,5,5,5] #每个 batch 的元素，由n_way 个classes组成\n",
    "classids=[0,1,2,3,4] #每个 batch 的元素，由n_way 个classes组成\n",
    "\n",
    "support_images,support_labels,query_images,query_labels=ds[classids]\n",
    "\n",
    "show_data(support_images,support_labels,num_way)\n",
    "show_data(query_images,query_labels,num_way)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea2a03c",
   "metadata": {},
   "source": [
    "## MAML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e28302",
   "metadata": {},
   "source": [
    "**Problems**\n",
    "- 1. In the maml.py file, complete the implementation of the MAML. inner loop and\n",
    "MAML. outer step methods. The former computes the task-adapted network parameters (and accuracy metrics), and the latter computes the MAML objective (and more metrics). Pay attention to the inline comments and docstrings.\n",
    "\n",
    "**Hint: the simplest way to implement inner loop involves using autograd.grad.**\n",
    "\n",
    "**Hint: read the documentation for the create graph argument of autograd.grad.**\n",
    "\n",
    "- 2. Assess your implementation of vanilla MAML on 5-way 1-shot Omniglot. Comments from the previous part regarding arguments, checkpoints, TensorBoard, resuming training, and testing all apply. Use 1 inner loop step with a fixed inner learning rate of 0.4. Use 15 query examples per class per task. You should not need to\n",
    "adjust the outer learning rate from its default of 0.001. Note that MAML generally\n",
    "needs more time to train than protonets.\n",
    "Submit a plot of the val post-adaptation query accuracy over the course of training.\n",
    "**Hint: you should obtain a query accuracy on the validation split of at least 93%.**\n",
    "- 3. Six accuracy metrics are logged. Examine these in detail to reason about what MAML is doing. Submit responses to the following questions:\n",
    "  - (a) What do you notice about the train pre adapt support and val pre adapt support\n",
    "accuracies? Why does this make sense given the task sampling process?\n",
    "  - (b) What can you infer about the model from comparing the train pre adapt support\n",
    "and train post adapt support accuracies? And the corresponding val accuracies?\n",
    "  - (c) What about by comparing the train post adapt support and train post adapt query\n",
    "accuracies? And the corresponding val accuracies?\n",
    "\n",
    "\n",
    "- 4. Try MAML with a fixed inner learning rate of 0.04. Submit a plot of the validation\n",
    "post-adaptation query accuracy over the course of training with for the two inner\n",
    "learning rates (0.04, 0.4). Submit a response to the following question: Why would\n",
    "these different values affect training?\n",
    "- 5. Try MAML with learning the inner learning rates. Initialize the inner learning rates\n",
    "with 0.4. Submit a plot of the validation post-adaptation query accuracy over the\n",
    "course of training for learning and not learning the inner learning rates, initialized\n",
    "at 0.4. Submit a response to the following question: What is the effect of learning\n",
    "the inner learning rates?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acab49e",
   "metadata": {
    "code_folding": [
     12,
     15,
     50,
     406
    ]
   },
   "outputs": [],
   "source": [
    "NUM_INPUT_CHANNELS = 1\n",
    "NUM_HIDDEN_CHANNELS = 64\n",
    "KERNEL_SIZE = 3\n",
    "NUM_CONV_LAYERS = 4\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "SUMMARY_INTERVAL = 10\n",
    "SAVE_INTERVAL = 100\n",
    "LOG_INTERVAL = 10\n",
    "VAL_INTERVAL = LOG_INTERVAL * 5\n",
    "NUM_TEST_TASKS = 600\n",
    "\n",
    "\n",
    "class MAML:\n",
    "    \"\"\"Trains and assesses a MAML.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_outputs,\n",
    "            num_inner_steps,\n",
    "            inner_lr,\n",
    "            learn_inner_lrs,\n",
    "            outer_lr,\n",
    "            log_dir\n",
    "    ):\n",
    "        \"\"\"Inits MAML.\n",
    "\n",
    "        The network consists of four convolutional blocks followed by a linear\n",
    "        head layer. Each convolutional block comprises a convolution layer, a\n",
    "        batch normalization layer, and ReLU activation.\n",
    "\n",
    "        Note that unlike conventional use, batch normalization is always done\n",
    "        with batch statistics, regardless of whether we are training or\n",
    "        evaluating. This technically makes meta-learning transductive, as\n",
    "        opposed to inductive.\n",
    "\n",
    "        Args:\n",
    "            num_outputs (int): dimensionality of output, i.e. number of classes\n",
    "                in a task\n",
    "            num_inner_steps (int): number of inner-loop optimization steps\n",
    "            inner_lr (float): learning rate for inner-loop optimization\n",
    "                If learn_inner_lrs=True, inner_lr serves as the initialization\n",
    "                of the learning rates.\n",
    "            learn_inner_lrs (bool): whether to learn the above\n",
    "            outer_lr (float): learning rate for outer-loop optimization\n",
    "            log_dir (str): path to logging directory\n",
    "        \"\"\"\n",
    "        meta_parameters = {}\n",
    "\n",
    "        # construct feature extractor\n",
    "        in_channels = NUM_INPUT_CHANNELS\n",
    "        for i in range(NUM_CONV_LAYERS):\n",
    "            meta_parameters[f'conv{i}'] = nn.init.xavier_uniform_(\n",
    "                torch.empty(\n",
    "                    NUM_HIDDEN_CHANNELS,\n",
    "                    in_channels,\n",
    "                    KERNEL_SIZE,\n",
    "                    KERNEL_SIZE,\n",
    "                    requires_grad=True,\n",
    "                    device=DEVICE\n",
    "                )\n",
    "            )\n",
    "            meta_parameters[f'b{i}'] = nn.init.zeros_(\n",
    "                torch.empty(\n",
    "                    NUM_HIDDEN_CHANNELS,\n",
    "                    requires_grad=True,\n",
    "                    device=DEVICE\n",
    "                )\n",
    "            )\n",
    "            in_channels = NUM_HIDDEN_CHANNELS\n",
    "\n",
    "        # construct linear head layer\n",
    "        meta_parameters[f'w{NUM_CONV_LAYERS}'] = nn.init.xavier_uniform_(\n",
    "            torch.empty(\n",
    "                num_outputs,\n",
    "                NUM_HIDDEN_CHANNELS,\n",
    "                requires_grad=True,\n",
    "                device=DEVICE\n",
    "            )\n",
    "        )\n",
    "        meta_parameters[f'b{NUM_CONV_LAYERS}'] = nn.init.zeros_(\n",
    "            torch.empty(\n",
    "                num_outputs,\n",
    "                requires_grad=True,\n",
    "                device=DEVICE\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self._meta_parameters = meta_parameters\n",
    "        self._num_inner_steps = num_inner_steps\n",
    "        self._inner_lrs = {\n",
    "            k: torch.tensor(inner_lr, requires_grad=learn_inner_lrs)\n",
    "            for k in self._meta_parameters.keys()\n",
    "        }\n",
    "        self._outer_lr = outer_lr\n",
    "\n",
    "        self._optimizer = torch.optim.Adam(\n",
    "            list(self._meta_parameters.values()) +\n",
    "            list(self._inner_lrs.values()),\n",
    "            lr=self._outer_lr\n",
    "        )\n",
    "        self._log_dir = log_dir\n",
    "        os.makedirs(self._log_dir, exist_ok=True)\n",
    "\n",
    "        self._start_train_step = 0\n",
    "\n",
    "    def _forward(self, images, parameters):\n",
    "        \"\"\"Computes predicted classification logits.\n",
    "\n",
    "        Args:\n",
    "            images (Tensor): batch of Omniglot images\n",
    "                shape (num_images, channels, height, width)\n",
    "            parameters (dict[str, Tensor]): parameters to use for\n",
    "                the computation\n",
    "\n",
    "        Returns:\n",
    "            a Tensor consisting of a batch of logits\n",
    "                shape (num_images, classes)\n",
    "        \"\"\"\n",
    "        x = images\n",
    "        for i in range(NUM_CONV_LAYERS):\n",
    "            x = F.conv2d(\n",
    "                input=x,\n",
    "                weight=parameters[f'conv{i}'],\n",
    "                bias=parameters[f'b{i}'],\n",
    "                stride=1,\n",
    "                padding='same'\n",
    "            )\n",
    "            x = F.batch_norm(x, None, None, training=True)\n",
    "            x = F.relu(x)\n",
    "        x = torch.mean(x, dim=[2, 3])\n",
    "        return F.linear(\n",
    "            input=x,\n",
    "            weight=parameters[f'w{NUM_CONV_LAYERS}'],\n",
    "            bias=parameters[f'b{NUM_CONV_LAYERS}']\n",
    "        )\n",
    "\n",
    "    def _inner_loop(self, images, labels, train):   # pylint: disable=unused-argument\n",
    "        \"\"\"Computes the adapted network parameters via the MAML inner loop.\n",
    "\n",
    "        Args:\n",
    "            images (Tensor): task support set inputs\n",
    "                shape (num_images, channels, height, width)\n",
    "            labels (Tensor): task support set outputs\n",
    "                shape (num_images,)\n",
    "            train (bool): whether we are training or evaluating\n",
    "\n",
    "        Returns:\n",
    "            parameters (dict[str, Tensor]): adapted network parameters\n",
    "            accuracies (list[float]): support set accuracy over the course of\n",
    "                the inner loop, length num_inner_steps + 1\n",
    "        \"\"\"\n",
    "        accuracies = []\n",
    "        parameters = {\n",
    "            k: torch.clone(v)   #虽然使用了clone,但是 backward的时候，赋值 clone(v)的grad的同时，会别此grad赋值给v,也就是 meta_parameter.\n",
    "            for k, v in self._meta_parameters.items()\n",
    "        }\n",
    "        # ********************************************************\n",
    "        # ******************* YOUR CODE HERE *********************\n",
    "        # ********************************************************\n",
    "        # TODO: finish implementing this method.\n",
    "        # This method computes the inner loop (adaptation) procedure for one\n",
    "        # task. It also scores the model along the way.\n",
    "        # Make sure to populate accuracies and update parameters.\n",
    "        # Use F.cross_entropy to compute classification losses.\n",
    "        # Use util.score to compute accuracies.\n",
    "\n",
    "        # ********************************************************\n",
    "        # ******************* YOUR CODE HERE *********************\n",
    "        # ********************************************************\n",
    "        \n",
    "        for step in range(self._num_inner_steps):\n",
    "            logits=self._forward(images,parameters)\n",
    "            accuracies.append(score(logits,labels))\n",
    "            \n",
    "            loss=F.cross_entropy(logits,labels)\n",
    "             \n",
    "            keys=parameters.keys()\n",
    "            values=parameters.values()\n",
    "            g=autograd.grad(loss,values,create_graph=True)\n",
    "            \n",
    "            parameters={\n",
    "                k:v-g[i]*self._inner_lrs[k] for i,(k,v) in enumerate(zip(keys,values)) \n",
    "            }\n",
    "        #最后一次更新完参数，计算准确率    \n",
    "        logits=self._forward(images,parameters)\n",
    "        accuracies.append(score(logits,labels))   \n",
    "        return parameters, accuracies\n",
    "\n",
    "    def _outer_step(self, task_batch, train):  # pylint: disable=unused-argument\n",
    "        \"\"\"Computes the MAML loss and metrics on a batch of tasks.\n",
    "\n",
    "        Args:\n",
    "            task_batch (tuple): batch of tasks from an Omniglot DataLoader\n",
    "            train (bool): whether we are training or evaluating\n",
    "\n",
    "        Returns:\n",
    "            outer_loss (Tensor): mean MAML loss over the batch, scalar\n",
    "            accuracies_support (ndarray): support set accuracy over the\n",
    "                course of the inner loop, averaged over the task batch\n",
    "                shape (num_inner_steps + 1,)\n",
    "            accuracy_query (float): query set accuracy of the adapted\n",
    "                parameters, averaged over the task batch\n",
    "        \"\"\"\n",
    "        outer_loss_batch = []\n",
    "        accuracies_support_batch = []\n",
    "        accuracy_query_batch = []\n",
    "        for task in task_batch:\n",
    "            images_support, labels_support, images_query, labels_query = task\n",
    "            images_support = images_support.to(DEVICE)\n",
    "            labels_support = labels_support.to(DEVICE)\n",
    "            images_query = images_query.to(DEVICE)\n",
    "            labels_query = labels_query.to(DEVICE)\n",
    "            # ********************************************************\n",
    "            # ******************* YOUR CODE HERE *********************\n",
    "            # ********************************************************\n",
    "            # TODO: finish implementing this method.\n",
    "            # For a given task, use the _inner_loop method to adapt, then\n",
    "            # compute the MAML loss and other metrics.\n",
    "            # Use F.cross_entropy to compute classification losses.\n",
    "            # Use util.score to compute accuracies.\n",
    "            # Make sure to populate outer_loss_batch, accuracies_support_batch,\n",
    "            # and accuracy_query_batch.\n",
    "\n",
    "            # ********************************************************\n",
    "            # ******************* YOUR CODE HERE *********************\n",
    "            # ********************************************************\n",
    "            \n",
    "            parameters,acc=self._inner_loop(images_support,labels_support,train)\n",
    "            accuracies_support_batch.append(acc)\n",
    "            \n",
    "            \n",
    "            logits=self._forward(images_query,parameters)\n",
    "            accuracy_query_batch.append(score(logits,labels_query))\n",
    "            \n",
    "            loss=F.cross_entropy(logits,labels_query)\n",
    "            outer_loss_batch.append(loss)\n",
    "            \n",
    "        outer_loss = torch.mean(torch.stack(outer_loss_batch))\n",
    "        accuracies_support = np.mean(\n",
    "            accuracies_support_batch,\n",
    "            axis=0\n",
    "        )\n",
    "        accuracy_query = np.mean(accuracy_query_batch)\n",
    "        return outer_loss, accuracies_support, accuracy_query\n",
    "\n",
    "    def train(self, dataloader_train, dataloader_val, writer):\n",
    "        \"\"\"Train the MAML.\n",
    "\n",
    "        Consumes dataloader_train to optimize MAML meta-parameters\n",
    "        while periodically validating on dataloader_val, logging metrics, and\n",
    "        saving checkpoints.\n",
    "\n",
    "        Args:\n",
    "            dataloader_train (DataLoader): loader for train tasks\n",
    "            dataloader_val (DataLoader): loader for validation tasks\n",
    "            writer (SummaryWriter): TensorBoard logger\n",
    "        \"\"\"\n",
    "        print(f'Starting training at iteration {self._start_train_step}.')\n",
    "        for i_step, task_batch in enumerate(dataloader_train,start=self._start_train_step):\n",
    "            self._optimizer.zero_grad()\n",
    "            outer_loss, accuracies_support, accuracy_query = (\n",
    "                self._outer_step(task_batch, train=True)\n",
    "            )\n",
    "            outer_loss.backward()\n",
    "            self._optimizer.step()\n",
    "\n",
    "            if i_step % LOG_INTERVAL == 0:\n",
    "                print(\n",
    "                    f'Iteration {i_step}: '\n",
    "                    f'loss: {outer_loss.item():.3f}, '\n",
    "                    f'pre-adaptation support accuracy: '\n",
    "                    f'{accuracies_support[0]:.3f}, '\n",
    "                    f'post-adaptation support accuracy: '\n",
    "                    f'{accuracies_support[-1]:.3f}, '\n",
    "                    f'post-adaptation query accuracy: '\n",
    "                    f'{accuracy_query:.3f}'\n",
    "                )\n",
    "                writer.add_scalar('loss/train', outer_loss.item(), i_step)\n",
    "                writer.add_scalar(\n",
    "                    'train_accuracy/pre_adapt_support',\n",
    "                    accuracies_support[0],\n",
    "                    i_step\n",
    "                )\n",
    "                writer.add_scalar(\n",
    "                    'train_accuracy/post_adapt_support',\n",
    "                    accuracies_support[-1],\n",
    "                    i_step\n",
    "                )\n",
    "                writer.add_scalar(\n",
    "                    'train_accuracy/post_adapt_query',\n",
    "                    accuracy_query,\n",
    "                    i_step\n",
    "                )\n",
    "\n",
    "            if i_step % VAL_INTERVAL == 0:\n",
    "                losses = []\n",
    "                accuracies_pre_adapt_support = []\n",
    "                accuracies_post_adapt_support = []\n",
    "                accuracies_post_adapt_query = []\n",
    "                for val_task_batch in dataloader_val:\n",
    "                    outer_loss, accuracies_support, accuracy_query = (\n",
    "                        self._outer_step(val_task_batch, train=False)\n",
    "                    )\n",
    "                    losses.append(outer_loss.item())\n",
    "                    accuracies_pre_adapt_support.append(accuracies_support[0])\n",
    "                    accuracies_post_adapt_support.append(accuracies_support[-1])\n",
    "                    accuracies_post_adapt_query.append(accuracy_query)\n",
    "                loss = np.mean(losses)\n",
    "                accuracy_pre_adapt_support = np.mean(\n",
    "                    accuracies_pre_adapt_support\n",
    "                )\n",
    "                accuracy_post_adapt_support = np.mean(\n",
    "                    accuracies_post_adapt_support\n",
    "                )\n",
    "                accuracy_post_adapt_query = np.mean(\n",
    "                    accuracies_post_adapt_query\n",
    "                )\n",
    "                print(\n",
    "                    f'Validation: '\n",
    "                    f'loss: {loss:.3f}, '\n",
    "                    f'pre-adaptation support accuracy: '\n",
    "                    f'{accuracy_pre_adapt_support:.3f}, '\n",
    "                    f'post-adaptation support accuracy: '\n",
    "                    f'{accuracy_post_adapt_support:.3f}, '\n",
    "                    f'post-adaptation query accuracy: '\n",
    "                    f'{accuracy_post_adapt_query:.3f}'\n",
    "                )\n",
    "                writer.add_scalar('loss/val', loss, i_step)\n",
    "                writer.add_scalar(\n",
    "                    'val_accuracy/pre_adapt_support',\n",
    "                    accuracy_pre_adapt_support,\n",
    "                    i_step\n",
    "                )\n",
    "                writer.add_scalar(\n",
    "                    'val_accuracy/post_adapt_support',\n",
    "                    accuracy_post_adapt_support,\n",
    "                    i_step\n",
    "                )\n",
    "                writer.add_scalar(\n",
    "                    'val_accuracy/post_adapt_query',\n",
    "                    accuracy_post_adapt_query,\n",
    "                    i_step\n",
    "                )\n",
    "\n",
    "            if i_step % SAVE_INTERVAL == 0:\n",
    "                self._save(i_step)\n",
    "\n",
    "    def test(self, dataloader_test):\n",
    "        \"\"\"Evaluate the MAML on test tasks.\n",
    "\n",
    "        Args:\n",
    "            dataloader_test (DataLoader): loader for test tasks\n",
    "        \"\"\"\n",
    "        accuracies = []\n",
    "        for task_batch in dataloader_test:\n",
    "            _, _, accuracy_query = self._outer_step(task_batch, train=False)\n",
    "            accuracies.append(accuracy_query)\n",
    "        mean = np.mean(accuracies)\n",
    "        std = np.std(accuracies)\n",
    "        mean_95_confidence_interval = 1.96 * std / np.sqrt(NUM_TEST_TASKS)\n",
    "        print(\n",
    "            f'Accuracy over {NUM_TEST_TASKS} test tasks: '\n",
    "            f'mean {mean:.3f}, '\n",
    "            f'95% confidence interval {mean_95_confidence_interval:.3f}'\n",
    "        )\n",
    "\n",
    "    def load(self, checkpoint_step):\n",
    "        \"\"\"Loads a checkpoint.\n",
    "\n",
    "        Args:\n",
    "            checkpoint_step (int): iteration of checkpoint to load\n",
    "\n",
    "        Raises:\n",
    "            ValueError: if checkpoint for checkpoint_step is not found\n",
    "        \"\"\"\n",
    "        target_path = (\n",
    "            f'{os.path.join(self._log_dir, \"state\")}'\n",
    "            f'{checkpoint_step}.pt'\n",
    "        )\n",
    "        if os.path.isfile(target_path):\n",
    "            state = torch.load(target_path)\n",
    "            self._meta_parameters = state['meta_parameters']\n",
    "            self._inner_lrs = state['inner_lrs']\n",
    "            self._optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "            self._start_train_step = checkpoint_step + 1\n",
    "            print(f'Loaded checkpoint iteration {checkpoint_step}.')\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f'No checkpoint for iteration {checkpoint_step} found.'\n",
    "            )\n",
    "\n",
    "    def _save(self, checkpoint_step):\n",
    "        \"\"\"Saves parameters and optimizer state_dict as a checkpoint.\n",
    "\n",
    "        Args:\n",
    "            checkpoint_step (int): iteration to label checkpoint with\n",
    "        \"\"\"\n",
    "        optimizer_state_dict = self._optimizer.state_dict()\n",
    "        torch.save(\n",
    "            dict(meta_parameters=self._meta_parameters,\n",
    "                 inner_lrs=self._inner_lrs,\n",
    "                 optimizer_state_dict=optimizer_state_dict),\n",
    "            f'{os.path.join(self._log_dir, \"state\")}{checkpoint_step}.pt'\n",
    "        )\n",
    "        print('Saved checkpoint.')\n",
    "\n",
    "def maml_main(args):\n",
    "    log_dir = args.log_dir\n",
    "    if log_dir is None:\n",
    "        log_dir = f'logs/maml/omniglot.way={args.num_way}.support={args.num_support}.query={args.num_query}.inner_steps={args.num_inner_steps}.inner_lr={args.inner_lr}.learn_inner_lrs={args.learn_inner_lrs}.outer_lr={args.outer_lr}.batch_size={args.batch_size}'  # pylint: disable=line-too-long\n",
    "    print(f'log_dir: {log_dir}')\n",
    "    writer = tensorboard.SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    maml = MAML(\n",
    "        args.num_way,\n",
    "        args.num_inner_steps,\n",
    "        args.inner_lr,\n",
    "        args.learn_inner_lrs,\n",
    "        args.outer_lr,\n",
    "        log_dir\n",
    "    )\n",
    "\n",
    "    if args.checkpoint_step > -1:\n",
    "        maml.load(args.checkpoint_step)\n",
    "    else:\n",
    "        print('Checkpoint loading skipped.')\n",
    "\n",
    "    if not args.test:\n",
    "        num_training_tasks = args.batch_size * (args.num_train_iterations -\n",
    "                                                args.checkpoint_step - 1)\n",
    "        print(\n",
    "            f'Training on {num_training_tasks} tasks with composition: '\n",
    "            f'num_way={args.num_way}, '\n",
    "            f'num_support={args.num_support}, '\n",
    "            f'num_query={args.num_query}'\n",
    "        )\n",
    "        dataloader_train = get_omniglot_dataloader(\n",
    "            'train',\n",
    "            args.batch_size,\n",
    "            args.num_way,\n",
    "            args.num_support,\n",
    "            args.num_query,\n",
    "            num_training_tasks\n",
    "        )\n",
    "        dataloader_val = get_omniglot_dataloader(\n",
    "            'val',\n",
    "            args.batch_size,\n",
    "            args.num_way,\n",
    "            args.num_support,\n",
    "            args.num_query,\n",
    "            args.batch_size * 4\n",
    "        )\n",
    "        maml.train(\n",
    "            dataloader_train,\n",
    "            dataloader_val,\n",
    "            writer\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            f'Testing on tasks with composition '\n",
    "            f'num_way={args.num_way}, '\n",
    "            f'num_support={args.num_support}, '\n",
    "            f'num_query={args.num_query}'\n",
    "        )\n",
    "        dataloader_test = get_omniglot_dataloader(\n",
    "            'test',\n",
    "            1,\n",
    "            args.num_way,\n",
    "            args.num_support,\n",
    "            args.num_query,\n",
    "            NUM_TEST_TASKS\n",
    "        )\n",
    "        maml.test(dataloader_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ad392a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 全局配置\n",
    "config={\n",
    "    \"num_way\":5,\n",
    "    \"num_support\":1,\n",
    "    \"num_query\":15,\n",
    "    \"learning_rate\":0.001,\n",
    "    \"batch_size\":16,\n",
    "    \"num_train_iterations\":3000, #15000\n",
    "    \"test\":False,\n",
    "    \"checkpoint_step\":-1,\n",
    "    \"log_dir\":None,\n",
    "    \"num_inner_steps\":1,\n",
    "    \"inner_lr\":0.4,\n",
    "    \"learn_inner_lrs\":False,\n",
    "    \"outer_lr\":0.001\n",
    "}\n",
    "config=getConfigObject(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0dbd694",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m config\u001b[39m.\u001b[39mlearn_inner_lrs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     11\u001b[0m config\u001b[39m.\u001b[39mcheckpoint_step\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m---> 12\u001b[0m maml_main(config)\n",
      "Cell \u001b[1;32mIn[6], line 453\u001b[0m, in \u001b[0;36mmaml_main\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m    437\u001b[0m     dataloader_train \u001b[39m=\u001b[39m get_omniglot_dataloader(\n\u001b[0;32m    438\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m    439\u001b[0m         args\u001b[39m.\u001b[39mbatch_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    443\u001b[0m         num_training_tasks\n\u001b[0;32m    444\u001b[0m     )\n\u001b[0;32m    445\u001b[0m     dataloader_val \u001b[39m=\u001b[39m get_omniglot_dataloader(\n\u001b[0;32m    446\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m    447\u001b[0m         args\u001b[39m.\u001b[39mbatch_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    451\u001b[0m         args\u001b[39m.\u001b[39mbatch_size \u001b[39m*\u001b[39m \u001b[39m4\u001b[39m\n\u001b[0;32m    452\u001b[0m     )\n\u001b[1;32m--> 453\u001b[0m     maml\u001b[39m.\u001b[39;49mtrain(\n\u001b[0;32m    454\u001b[0m         dataloader_train,\n\u001b[0;32m    455\u001b[0m         dataloader_val,\n\u001b[0;32m    456\u001b[0m         writer\n\u001b[0;32m    457\u001b[0m     )\n\u001b[0;32m    458\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    459\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    460\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTesting on tasks with composition \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    461\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnum_way=\u001b[39m\u001b[39m{\u001b[39;00margs\u001b[39m.\u001b[39mnum_way\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    462\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnum_support=\u001b[39m\u001b[39m{\u001b[39;00margs\u001b[39m.\u001b[39mnum_support\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    463\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnum_query=\u001b[39m\u001b[39m{\u001b[39;00margs\u001b[39m.\u001b[39mnum_query\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m    464\u001b[0m     )\n",
      "Cell \u001b[1;32mIn[6], line 262\u001b[0m, in \u001b[0;36mMAML.train\u001b[1;34m(self, dataloader_train, dataloader_val, writer)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[39mfor\u001b[39;00m i_step, task_batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader_train,start\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_train_step):\n\u001b[0;32m    260\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m    261\u001b[0m     outer_loss, accuracies_support, accuracy_query \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 262\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_outer_step(task_batch, train\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    263\u001b[0m     )\n\u001b[0;32m    264\u001b[0m     outer_loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m    265\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer\u001b[39m.\u001b[39mstep()\n",
      "Cell \u001b[1;32mIn[6], line 233\u001b[0m, in \u001b[0;36mMAML._outer_step\u001b[1;34m(self, task_batch, train)\u001b[0m\n\u001b[0;32m    229\u001b[0m accuracies_support_batch\u001b[39m.\u001b[39mappend(acc)\n\u001b[0;32m    232\u001b[0m logits\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward(images_query,parameters)\n\u001b[1;32m--> 233\u001b[0m accuracy_query_batch\u001b[39m.\u001b[39mappend(score(logits,labels_query))\n\u001b[0;32m    235\u001b[0m loss\u001b[39m=\u001b[39mF\u001b[39m.\u001b[39mcross_entropy(logits,labels_query)\n\u001b[0;32m    236\u001b[0m outer_loss_batch\u001b[39m.\u001b[39mappend(loss)\n",
      "Cell \u001b[1;32mIn[3], line 16\u001b[0m, in \u001b[0;36mscore\u001b[1;34m(logits, labels)\u001b[0m\n\u001b[0;32m     14\u001b[0m y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(logits, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m==\u001b[39m labels\n\u001b[0;32m     15\u001b[0m y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mtype(torch\u001b[39m.\u001b[39mfloat)\n\u001b[1;32m---> 16\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mmean(y)\u001b[39m.\u001b[39;49mitem()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#训练网络\n",
    "SEED_CLASS=None # 每次取出的task对于的类别确定\n",
    "SEED_IMAGE=None #如果是相同的class,每次取出的图片确定\n",
    "### train network\n",
    "config.num_way=5\n",
    "config.num_support=1\n",
    "config.num_query=15\n",
    "config.test=False\n",
    "config.inner_lr=0.4\n",
    "config.learn_inner_lrs=True\n",
    "config.checkpoint_step=-1\n",
    "maml_main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54f7e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.num_way=5\n",
    "# config.num_support=1\n",
    "# config.num_query=15\n",
    "config.test=True\n",
    "config.checkpoint_step=2900\n",
    "maml_main(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1409c11c",
   "metadata": {},
   "source": [
    "inner_lr=0.4.learn_inner_lrs=False：\n",
    "\n",
    "Testing on tasks with composition num_way=5, num_support=1, num_query=15\n",
    "Accuracy over 600 test tasks: mean 0.966, 95% confidence interval 0.003\n",
    "\n",
    "inner_lr=0.04.learn_inner_lrs=False：\n",
    "Testing on tasks with composition num_way=5, num_support=1, num_query=15\n",
    "Accuracy over 600 test tasks: mean 0.953, 95% confidence interval 0.00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c19908d",
   "metadata": {},
   "source": [
    "### ProtoNet\n",
    "**Problem**:\n",
    "\n",
    "- 1. In the protonet.py file, complete the implementation of the ProtoNet. step method,\n",
    "which computes (5) along with accuracy metrics. Pay attention to the inline comments and docstrings.\n",
    "- 2. Assess your implementation on 5-way 5-shot Omniglot。 Use 15 query examples per class per task. Depending on how much memory your\n",
    "GPU has, you may need to reduce the batch size. You should not need to adjust the\n",
    "learning rate from its default of 0.001.\n",
    "**Hint: you should obtain a query accuracy on the validation split of at least 97%.**\n",
    "\n",
    "- 3. Four accuracy metrics are logged. For the above run, examine these in detail to reason about what the algorithm is doing. Submit responses to the following questions:\n",
    "  - (a) What do you notice about the train support and val support accuracy? What\n",
    "does this suggest about where the protonet places support examples of the same\n",
    "class in feature space?\n",
    "  - (b) Compare train query and val query. Is the model generalizing to new tasks?\n",
    "If not, is it overfitting or underfitting?\n",
    "- 4. Train on 5-way 1-shot tasks. Submit a table comparing test performance on 5-way\n",
    "1-shot tasks, with 95% confidence intervals, between training on 5-way 1-shot vs. 5-\n",
    "way 5-shot tasks. Submit responses to the following questions: How did you choose\n",
    "which checkpoint to use for testing for each model? What do you notice about the\n",
    "test performance? If there is a difference, what could explain this difference?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e7ad82",
   "metadata": {
    "code_folding": [
     12,
     62,
     83,
     158,
     187,
     318
    ]
   },
   "outputs": [],
   "source": [
    "NUM_INPUT_CHANNELS = 1\n",
    "NUM_HIDDEN_CHANNELS = 64\n",
    "KERNEL_SIZE = 3\n",
    "NUM_CONV_LAYERS = 4\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "SUMMARY_INTERVAL = 10\n",
    "SAVE_INTERVAL = 100\n",
    "PRINT_INTERVAL = 10\n",
    "VAL_INTERVAL = PRINT_INTERVAL * 5\n",
    "NUM_TEST_TASKS = 600\n",
    "\n",
    "\n",
    "class ProtoNetNetwork(nn.Module):\n",
    "    \"\"\"Container for ProtoNet weights and image-to-latent computation.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Inits ProtoNetNetwork.\n",
    "\n",
    "        The network consists of four convolutional blocks, each comprising a\n",
    "        convolution layer, a batch normalization layer, ReLU activation, and 2x2\n",
    "        max pooling for downsampling. There is an additional flattening\n",
    "        operation at the end.\n",
    "\n",
    "        Note that unlike conventional use, batch normalization is always done\n",
    "        with batch statistics, regardless of whether we are training or\n",
    "        evaluating. This technically makes meta-learning transductive, as\n",
    "        opposed to inductive.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_channels = NUM_INPUT_CHANNELS\n",
    "        for _ in range(NUM_CONV_LAYERS):\n",
    "            layers.append(\n",
    "                nn.Conv2d(\n",
    "                    in_channels,\n",
    "                    NUM_HIDDEN_CHANNELS,\n",
    "                    (KERNEL_SIZE, KERNEL_SIZE),\n",
    "                    padding='same'\n",
    "                )\n",
    "            )\n",
    "            layers.append(nn.BatchNorm2d(NUM_HIDDEN_CHANNELS))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.MaxPool2d(2))\n",
    "            in_channels = NUM_HIDDEN_CHANNELS\n",
    "        layers.append(nn.Flatten())\n",
    "        self._layers = nn.Sequential(*layers)\n",
    "        self.to(DEVICE)\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"Computes the latent representation of a batch of images.\n",
    "\n",
    "        Args:\n",
    "            images (Tensor): batch of Omniglot images\n",
    "                shape (num_images, channels, height, width)\n",
    "\n",
    "        Returns:\n",
    "            a Tensor containing a batch of latent representations\n",
    "                shape (num_images, latents)\n",
    "        \"\"\"\n",
    "        return self._layers(images)\n",
    "\n",
    "\n",
    "class ProtoNet:\n",
    "    \"\"\"Trains and assesses a prototypical network.\"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate, log_dir):\n",
    "        \"\"\"Inits ProtoNet.\n",
    "\n",
    "        Args:\n",
    "            learning_rate (float): learning rate for the Adam optimizer\n",
    "            log_dir (str): path to logging directory\n",
    "        \"\"\"\n",
    "\n",
    "        self._network = ProtoNetNetwork()\n",
    "        self._optimizer = torch.optim.Adam(\n",
    "            self._network.parameters(),\n",
    "            lr=learning_rate\n",
    "        )\n",
    "        self._log_dir = log_dir\n",
    "        os.makedirs(self._log_dir, exist_ok=True)\n",
    "\n",
    "        self._start_train_step = 0\n",
    "\n",
    "    def _step(self, task_batch):\n",
    "        \"\"\"Computes ProtoNet mean loss (and accuracy) on a batch of tasks.\n",
    "\n",
    "        Args:\n",
    "            task_batch (tuple[Tensor, Tensor, Tensor, Tensor]):\n",
    "                batch of tasks from an Omniglot DataLoader\n",
    "\n",
    "        Returns:\n",
    "            a Tensor containing mean ProtoNet loss over the batch\n",
    "                shape ()\n",
    "            mean support set accuracy over the batch as a float\n",
    "            mean query set accuracy over the batch as a float\n",
    "        \"\"\"\n",
    "        loss_batch = []\n",
    "        accuracy_support_batch = []\n",
    "        accuracy_query_batch = []\n",
    "        for task in task_batch:\n",
    "            images_support, labels_support, images_query, labels_query = task\n",
    "            images_support = images_support.to(DEVICE) #(K*N,1,28,28)\n",
    "            labels_support = labels_support.to(DEVICE) #(K*N,1)\n",
    "            images_query = images_query.to(DEVICE)   #(Q*N,1,28,28)\n",
    "            labels_query = labels_query.to(DEVICE)   #(Q*N,)\n",
    "            # ********************************************************\n",
    "            # ******************* YOUR CODE HERE *********************\n",
    "            # ********************************************************\n",
    "            # TODO: finish implementing this method.\n",
    "            # For a given task, compute the prototypes and the protonet loss.\n",
    "            # Use F.cross_entropy to compute classification losses.\n",
    "            # Use util.score to compute accuracies.\n",
    "            # Make sure to populate loss_batch, accuracy_support_batch, and\n",
    "            # accuracy_query_batch.\n",
    "\n",
    "            # ********************************************************\n",
    "            # ******************* YOUR CODE HERE *********************\n",
    "            # ********************************************************\n",
    "\n",
    "            N,K,Q=config.num_way,config.num_support,config.num_query\n",
    "            \n",
    "            f=self._network\n",
    "            feature_support=f(images_support)\n",
    "            feature_query=f(images_query)\n",
    "            assert tuple(feature_support.shape[:-1])==(K*N,)\n",
    "            assert tuple(feature_query.shape[:-1])==(Q*N,)\n",
    "            \n",
    "            # this is prototypes of classes\n",
    "            center_support=feature_support.view(N,K,-1).mean(1,keepdim=True)\n",
    "            center_support.transpose_(0,1)\n",
    "            assert tuple(center_support.shape[:-1])==(1,N)\n",
    "            \n",
    "            \n",
    "            # loss,accuracy for query set\n",
    "            feature_query=torch.unsqueeze(feature_query,dim=1)\n",
    "            assert tuple(feature_query.shape[:-1])==(Q*N,1)\n",
    "\n",
    "            query_logits=-(center_support-feature_query).square().sum(dim=-1)\n",
    "            assert tuple(query_logits.shape)==(Q*N,N)\n",
    "\n",
    "            loss=F.cross_entropy(query_logits,labels_query)\n",
    "            acc_query=score(query_logits,labels_query)\n",
    "            loss_batch.append(loss)\n",
    "            accuracy_query_batch.append(acc_query)\n",
    "            \n",
    "            #accuray for support set \n",
    "            feature_support=torch.unsqueeze(feature_support,dim=1)\n",
    "            assert tuple(feature_support.shape[:-1])==(K*N,1)\n",
    "            support_logits=-(center_support-feature_support).square().sum(dim=-1)\n",
    "            assert tuple(support_logits.shape)==(K*N,N)\n",
    "            acc_support=score(support_logits,labels_support)\n",
    "            accuracy_support_batch.append(acc_support)\n",
    "        return (\n",
    "            torch.mean(torch.stack(loss_batch)),\n",
    "            np.mean(accuracy_support_batch),\n",
    "            np.mean(accuracy_query_batch)\n",
    "        )\n",
    "\n",
    "    def predict(self,task):\n",
    "        with torch.no_grad():\n",
    "            images_support, labels_support, images_query, labels_query = task\n",
    "            images_support = images_support.to(DEVICE) #(K*N,1,28,28)\n",
    "            labels_support = labels_support.to(DEVICE) #(K*N,1)\n",
    "            images_query = images_query.to(DEVICE)   #(Q*N,1,28,28)\n",
    "            labels_query = labels_query.to(DEVICE)\n",
    "            \n",
    "            N,K,Q=config.num_way,config.num_support,config.num_query\n",
    "                \n",
    "            f=self._network\n",
    "            feature_support=f(images_support)\n",
    "            feature_query=f(images_query)\n",
    "            assert tuple(feature_support.shape[:-1])==(K*N,)\n",
    "            assert tuple(feature_query.shape[:-1])==(Q*N,)\n",
    "                \n",
    "            # this is prototypes of classes\n",
    "            center_support=feature_support.view(N,K,-1).mean(1,keepdim=True)\n",
    "            center_support.transpose_(0,1)\n",
    "            assert tuple(center_support.shape[:-1])==(1,N)\n",
    "            \n",
    "            # loss,accuracy for query set\n",
    "            feature_query=torch.unsqueeze(feature_query,dim=1)\n",
    "            assert tuple(feature_query.shape[:-1])==(Q*N,1)\n",
    "\n",
    "            query_logits=-(center_support-feature_query).square().sum(dim=-1)\n",
    "            assert tuple(query_logits.shape)==(Q*N,N)\n",
    "            \n",
    "            return query_logits.argmax(dim=-1)\n",
    "    def train(self, dataloader_train, dataloader_val, writer):\n",
    "        \"\"\"Train the ProtoNet.\n",
    "\n",
    "        Consumes dataloader_train to optimize weights of ProtoNetNetwork\n",
    "        while periodically validating on dataloader_val, logging metrics, and\n",
    "        saving checkpoints.\n",
    "\n",
    "        Args:\n",
    "            dataloader_train (DataLoader): loader for train tasks\n",
    "            dataloader_val (DataLoader): loader for validation tasks\n",
    "            writer (SummaryWriter): TensorBoard logger\n",
    "        \"\"\"\n",
    "        print(f'Starting training at iteration {self._start_train_step}.')\n",
    "        for i_step, task_batch in enumerate(\n",
    "                dataloader_train,\n",
    "                start=self._start_train_step\n",
    "        ):\n",
    "            self._optimizer.zero_grad()\n",
    "            loss, accuracy_support, accuracy_query = self._step(task_batch)\n",
    "            loss.backward()\n",
    "            self._optimizer.step()\n",
    "\n",
    "            if i_step % PRINT_INTERVAL == 0:\n",
    "                print(\n",
    "                    f'Iteration {i_step}: '\n",
    "                    f'loss: {loss.item():.3f}, '\n",
    "                    f'support accuracy: {accuracy_support.item():.3f}, '\n",
    "                    f'query accuracy: {accuracy_query.item():.3f}'\n",
    "                )\n",
    "                writer.add_scalar('loss/train', loss.item(), i_step)\n",
    "                writer.add_scalar(\n",
    "                    'train_accuracy/support',\n",
    "                    accuracy_support.item(),\n",
    "                    i_step\n",
    "                )\n",
    "                writer.add_scalar(\n",
    "                    'train_accuracy/query',\n",
    "                    accuracy_query.item(),\n",
    "                    i_step\n",
    "                )\n",
    "\n",
    "            if i_step % VAL_INTERVAL == 0:\n",
    "                with torch.no_grad():\n",
    "                    losses, accuracies_support, accuracies_query = [], [], []\n",
    "                    for val_task_batch in dataloader_val:\n",
    "                        loss, accuracy_support, accuracy_query = (\n",
    "                            self._step(val_task_batch)\n",
    "                        )\n",
    "                        losses.append(loss.item())\n",
    "                        accuracies_support.append(accuracy_support)\n",
    "                        accuracies_query.append(accuracy_query)\n",
    "                    loss = np.mean(losses)\n",
    "                    accuracy_support = np.mean(accuracies_support)\n",
    "                    accuracy_query = np.mean(accuracies_query)\n",
    "                print(\n",
    "                    f'Validation: '\n",
    "                    f'loss: {loss:.3f}, '\n",
    "                    f'support accuracy: {accuracy_support:.3f}, '\n",
    "                    f'query accuracy: {accuracy_query:.3f}'\n",
    "                )\n",
    "        \n",
    "                writer.add_scalar('loss/val', loss, i_step)\n",
    "                writer.add_scalar(\n",
    "                    'val_accuracy/support',\n",
    "                    accuracy_support,\n",
    "                    i_step\n",
    "                )\n",
    "                writer.add_scalar(\n",
    "                    'val_accuracy/query',\n",
    "                    accuracy_query,\n",
    "                    i_step\n",
    "                )\n",
    "\n",
    "            if i_step % SAVE_INTERVAL == 0:\n",
    "                self._save(i_step)\n",
    "\n",
    "    def test(self, dataloader_test):\n",
    "        \"\"\"Evaluate the ProtoNet on test tasks.\n",
    "\n",
    "        Args:\n",
    "            dataloader_test (DataLoader): loader for test tasks\n",
    "        \"\"\"\n",
    "        accuracies = []\n",
    "        for task_batch in dataloader_test:\n",
    "            accuracies.append(self._step(task_batch)[2])\n",
    "        mean = np.mean(accuracies)\n",
    "        std = np.std(accuracies)\n",
    "        mean_95_confidence_interval = 1.96 * std / np.sqrt(NUM_TEST_TASKS)\n",
    "        print(\n",
    "            f'Accuracy over {NUM_TEST_TASKS} test tasks: '\n",
    "            f'mean {mean:.3f}, '\n",
    "            f'95% confidence interval {mean_95_confidence_interval:.3f}'\n",
    "        )\n",
    "\n",
    "    def load(self, checkpoint_step):\n",
    "        \"\"\"Loads a checkpoint.\n",
    "\n",
    "        Args:\n",
    "            checkpoint_step (int): iteration of checkpoint to load\n",
    "\n",
    "        Raises:\n",
    "            ValueError: if checkpoint for checkpoint_step is not found\n",
    "        \"\"\"\n",
    "        target_path = (\n",
    "            f'{os.path.join(self._log_dir, \"state\")}'\n",
    "            f'{checkpoint_step}.pt'\n",
    "        )\n",
    "        if os.path.isfile(target_path):\n",
    "            state = torch.load(target_path)\n",
    "            self._network.load_state_dict(state['network_state_dict'])\n",
    "            self._optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "            self._start_train_step = checkpoint_step + 1\n",
    "            print(f'Loaded checkpoint iteration {checkpoint_step}.')\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f'No checkpoint for iteration {checkpoint_step} found.'\n",
    "            )\n",
    "\n",
    "    def _save(self, checkpoint_step):\n",
    "        \"\"\"Saves network and optimizer state_dicts as a checkpoint.\n",
    "\n",
    "        Args:\n",
    "            checkpoint_step (int): iteration to label checkpoint with\n",
    "        \"\"\"\n",
    "        torch.save(\n",
    "            dict(network_state_dict=self._network.state_dict(),\n",
    "                 optimizer_state_dict=self._optimizer.state_dict()),\n",
    "            f'{os.path.join(self._log_dir, \"state\")}{checkpoint_step}.pt'\n",
    "        )\n",
    "        print('Saved checkpoint.')\n",
    "        \n",
    "def protonet_main(args):\n",
    "    log_dir = args.log_dir\n",
    "    if log_dir is None:\n",
    "        log_dir = f'logs/protonet/omniglot.way={args.num_way}.support={args.num_support}.query={args.num_query}.lr={args.learning_rate}.batch_size={args.batch_size}'  # pylint: disable=line-too-long\n",
    "    print(f'log_dir: {log_dir}')\n",
    "    writer = tensorboard.SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    protonet = ProtoNet(args.learning_rate, log_dir)\n",
    "\n",
    "    if args.checkpoint_step > -1:\n",
    "        protonet.load(args.checkpoint_step)\n",
    "    else:\n",
    "        print('Checkpoint loading skipped.')\n",
    "\n",
    "    if not args.test:\n",
    "        num_training_tasks = args.batch_size * (args.num_train_iterations -\n",
    "                                                args.checkpoint_step - 1)\n",
    "        print(\n",
    "            f'Training on tasks with composition '\n",
    "            f'num_way={args.num_way}, '\n",
    "            f'num_support={args.num_support}, '\n",
    "            f'num_query={args.num_query}'\n",
    "        )\n",
    "        dataloader_train = get_omniglot_dataloader(\n",
    "            'train',\n",
    "            args.batch_size,\n",
    "            args.num_way,\n",
    "            args.num_support,\n",
    "            args.num_query,\n",
    "            num_training_tasks\n",
    "        )\n",
    "        dataloader_val = get_omniglot_dataloader(\n",
    "            'val',\n",
    "            args.batch_size,\n",
    "            args.num_way,\n",
    "            args.num_support,\n",
    "            args.num_query,\n",
    "            args.batch_size * 4\n",
    "        )\n",
    "        protonet.train(\n",
    "            dataloader_train,\n",
    "            dataloader_val,\n",
    "            writer\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            f'Testing on tasks with composition '\n",
    "            f'num_way={args.num_way}, '\n",
    "            f'num_support={args.num_support}, '\n",
    "            f'num_query={args.num_query}'\n",
    "        )\n",
    "        dataloader_test = get_omniglot_dataloader(\n",
    "            'test',\n",
    "            1,\n",
    "            args.num_way,\n",
    "            args.num_support,\n",
    "            args.num_query,\n",
    "            NUM_TEST_TASKS\n",
    "        )\n",
    "        protonet.test(dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e678967",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\n",
    "    \"num_way\":5,\n",
    "    \"num_support\":5,\n",
    "    \"num_query\":15,\n",
    "    \"learning_rate\":0.001,\n",
    "    \"batch_size\":16,\n",
    "    \"num_train_iterations\":3000, #15000\n",
    "    \"test\":False,\n",
    "    \"checkpoint_step\":-1,\n",
    "    \"log_dir\":None\n",
    "}\n",
    "config=getConfigObject(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2bc5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### train network\n",
    "SEED_CLASS=None # 每次取出的task对于的类别确定\n",
    "SEED_IMAGE=None #如果是相同的class,每次取出的图片确定\n",
    "\n",
    "config.num_way=5\n",
    "config.num_support=5\n",
    "config.num_query=15\n",
    "config.test=False\n",
    "config.checkpoint_step=-1\n",
    "protonet_main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4c2b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "### test network\n",
    "#尽可能确保数据集一样\n",
    "SEED_CLASS=1215151 # 每次取出的task对于的类别确定\n",
    "SEED_IMAGE=45451 #如果是相同的class,每次取出的图片确定\n",
    "\n",
    "\n",
    "config.test=True\n",
    "config.log_dir='logs/protonet/omniglot.way=5.support=5.query=15.lr=0.001.batch_size=16'\n",
    "config.num_support=4  #这里可以改成不同的shot值\n",
    "config.num_query=15\n",
    "config.checkpoint_step=900  #选择最好的val \n",
    "protonet_main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d9473a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED_CLASS=None # 每次取出的task对于的类别确定\n",
    "SEED_IMAGE=None #如果是相同的class,每次取出的图片确定\n",
    "\n",
    "\n",
    "config.num_support=1\n",
    "config.num_query=5\n",
    "config.num_way=5\n",
    "model=ProtoNet(0,'logs/protonet/omniglot.way=5.support=5.query=15.lr=0.001.batch_size=16')\n",
    "test_dataloader=get_omniglot_dataloader('test',1,config.num_way,config.num_support,config.num_query,NUM_TEST_TASKS)\n",
    "model.load(900)\n",
    "\n",
    "# model.test(test_dataloader)\n",
    "for i,batch_data in enumerate(test_dataloader):\n",
    "    s_im,s_lb,q_im,q_lb=batch_data[0]\n",
    "    show_data(s_im,s_lb,config.num_way)\n",
    "    \n",
    "    predict_lb=model.predict(batch_data[0])\n",
    "    show_data(q_im,predict_lb,config.num_way)\n",
    "    print((predict_lb.cpu()==q_lb).float().mean())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b789d48e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
